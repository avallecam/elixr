---
title: "from Template to Standardized ELISA"
author: "Andree Valle Campos"
date: '`r Sys.Date()`'
output: 
  html_document:
#  pdf_document:
#  html_notebook:
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
    code_folding: "hide"
#    number_sections: TRUE
#    df_print: kable
#    fig_caption: true
#  documentclass: report
bibliography: SeroMarker.bib
csl: american-medical-association.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, # CHANGE TO FALSE IN PDF_DOCUMENTS
                      warning = FALSE) 
knitr::opts_knit$set(root.dir = gsub("/analysis","",getwd()))
#getwd()
options(width = 110) # CHANGE TO DEFAULT in PDF
# FOR PDF CHANGE: html_document, toc_float, number_sections, echo, width
# FOR PDF add new page USE: four \newpage BEFORE #SETUP #APPENDIX #COMPUTER.env #REFERENCES
```

# Setup 

This report is writen in **R** [@R] within a `Rmarkdown` [@rmarkdown] notebook using the `RStudio` [@rstudio] IDE software, and employing the `knitr` [@knitr] and `Hmisc` [@Hmisc] packages for the `html` setup.

This dynamic document integrates **text**, **code** and **results**. For more information, I recommend a recent 2016 review in spanish about **Reproducible science: what, why, how?** [@CienciaReproducible2016].

```{r setup0, results='hide', message=FALSE, eval=TRUE}
require(Hmisc)
#require(plotly)
#options(#grType='plotly',  # for certain graphics functions
#        width = 110) # to expand the limits of CONSOLE output
mu <- markupSpecs$html   # markupSpecs is in Hmisc

# The following hidden command (<code>r mu$widescreen()</code>), causes the html notebook to use an entire wide screen.
#mu$widescreen()
```
`r mu$widescreen()`

# Summary

**OBJECTIVE:**

* Principal: 
    - Standardize the OD~450nm~ values across ELISA plates by the estimation of the Antibody Units of each unknown sample.

* Secondary:
    - Implement a [reproducible workflow](https://www.ncbi.nlm.nih.gov/pubmed/26776185) for standardization of ELISA plates from a template matrix.

**SOLVED PROBLEMS:**


*01feb2017:*

- **To discuss:** Criteria for NaN:
    + If approaching zero, then change NaN to 0? (n=46+6)
    + If aproaching infinity, then dilute and re-run? (n=3)

- Covariate [labels](#new-db-stats) could be changes if required.

- Covariates visualization of the 03 samples requiring a [re-run](#re-run).

- Project folder arrangement ready to share via *Git(Hub)*[@CienciaReproducible2016].

**NEXT STEPS:**

- Writing of methodology and its limitations for a potential publication.

- Replicate seropositive analysis.
    + Check if "NaN" data cells creates dificulties. (easy to correct)
    + Contrast mean.OD and standardized Ab.units serpositivity patters per community.

- Share code via [Git(Hub)](https://speakerdeck.com/alicebartlett/git-for-humans) with Nuno.
    + BENEFITS:
        + Will improve collaboration:
            + code review, version control, error tracking and paper writing.
        + Will improve publication process: 
            + journal peer-review process, preference on [plain-text manustripts](https://github.com/rstudio/rticles) with code available.
        + Will improve publication quality and impact:
            + sharing a reproducible workflow acelerates scientific progress
                + Remember that standardized open-access procedures are still a deficit in this field.
                + Recent "open-acces" publications by Biggs[@biggs2017] and Dewasurendra [@dewasurendra2017effectiveness] *do not* provide open-acces methods 
                + only in the last[@dewasurendra2017effectiveness] specifies that "datasets and scripts are available under reasonable request".
            + This type of reports, relevant for malaria surveillance[@elliott2014surveillance], would be highly improved with standardize open-acess lab/comp protocols.
        + Will ease the transition from the Rproject to a Rpackage aim to analyse this specific data.
            + Nuno have already made most of the require methods[@Seplveda2015] and simulations[@sepulveda2015sample] with open-access tools.
        


# Introduction

This is an R implementation of a pipeline for the standardization of ELISA plates directly from a template matrix.


# Dependencies

The required R packages for this analysis are:

+ XLConnect [@XLConnect]
+ drc [@Ritz2015]

```{r, results='hide', message=FALSE}
##essential
library(XLConnect) # to load EXCEL files
library(drc)       # Dose-Response modeling
##accesory
library("DiagrammeR") # method Flowchart
#library("rcompanion") # pseudo R-2
library("knitr")     # To display nice tables
##extra
library("ggplot2")
library("Rmisc")        #multiploting ggplots
```

# Method

```{r, fig.align='center', fig.width=9}
#install.packages("DiagrammeR")
#library(DiagrammeR)

DiagrammeR("
  graph LR
    A[XLS data] -.-> |XLConnect| B{R data}
    B --> C1[STD]
    B --> E[ctr +/-]
    B --> D1[UNK]
    
    C1 -.-> |drc| C2[4pLL model]
    C2 --> C3[Box-Cox]
    C3 --> F{UNK Ab.units}

    D1 --> D2[mean.OD]
    D2 --> D3[OD %CV]
    D3 --> F
    
    F --> G1[Histogram]
    F --> G2[Density]
    F --> G3[QQPlot]

    style A fill:#ffffff, stroke:#000000, stroke-width:2px
    style B fill:#ffffff, stroke:#000000, stroke-width:2px
    style C1 fill:#ffffff, stroke:#000000, stroke-width:2px
    style C2 fill:#ffffff, stroke:#000000, stroke-width:2px
    style C3 fill:#ffffff, stroke:#000000, stroke-width:2px
    style D1 fill:#ffffff, stroke:#000000, stroke-width:2px
    style D2 fill:#ffffff, stroke:#000000, stroke-width:2px
    style D3 fill:#ffffff, stroke:#000000, stroke-width:2px
    style E fill:#ffffff, stroke:#000000, stroke-width:2px
    style F fill:#ffffff, stroke:#000000, stroke-width:2px
    style G1 fill:#ffffff, stroke:#000000, stroke-width:2px
    style G2 fill:#ffffff, stroke:#000000, stroke-width:2px
    style G3 fill:#ffffff, stroke:#000000, stroke-width:2px
")
```

## Log-Logistic (4pLL) model

The curve of the **log-logistic symetric** model describe the *response* `f(x)` dependent of the *dose* `x` and **04 parameters**: $$ f(x)=f(x;b,c,d,e)=c+\frac{d-c}{1+\exp[b(log(x)-log(e))]}\ $$ where: 
  
- `c` is the **lower limit** of the response when the *dose* `x` approaches infinity, 
- `d` is the **upper limit** when the *dose* `x` approaches zero,
- `b` is the **slope** around the **point of inflection**, represented by 
- `e` defined as **effective dose** and commmonly denoted as [@Ritz2015]:
    + `ED50`, `EC50` or `IC50` for continuous responses,
    + `LD50` or `LC50` for binomial responses, and
    + $T_{50}$ for event-time responses.

# Procedure

**12 summary plots** per ELISA Template:

- **3x3 plots** of STD and UNK distribution, residual variance distribution, and model transformation.
- **1x3 plots** of OD~450nm~, mean.OD and Ab.units distributions by Density plots.


```{r}
#
## CREATE A LIST WITH THE FILE.NAMES
#
a <- "Template_ELISA_N"
b <- seq(11,41,1)
b <- b[-c(3,4,9)] # NO TEMPLATE AVAILABLE
c <- ".xlsx"
#
d <- paste0(a,b,c)
#d[j]
data.frame(d)
#
e <- rep("data-raw/",length(d))
#e <- rep("~/Documents/Valle_GnB/000_R_script_MIRRORS/R_test/SeroMarkerMir/data-raw/",length(d))
d <- paste0(e,d)
```

```{r}
## A NULL DATA.FRAME TO FEED WITH DATA.ANALYSIS OUTCOMES
wb11_Ab <- data.frame()
paramALL <- data.frame()
```

```{r, fig.align='center', fig.height=12, fig.width=9}
#
## A LOOP TO ANALYSE EACH TEMPLATE
#
for (j in 1:length(d)) {
#
####
#### START ANALYSIS
####
# 1 UPLOAD excel data
wb11 <- loadWorkbook(d[j])
#
wb11_Pf_main <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 22, endRow = 30,
                         startCol = 3, endCol = 14)
wb11_Pf <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 33, endRow = 41,
                         startCol = 3, endCol = 14)
wb11_Pv_main <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 79, endRow = 87,
                         startCol = 3, endCol = 14)
wb11_Pv <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 90, endRow = 98,
                         startCol = 3, endCol = 14)
# 2 GENERATE R DATA.FRAME
Pf.OD <- matrix(wb11_Pf[1,])
Pf.ID <- matrix(wb11_Pf_main[1,])
Pv.OD <- matrix(wb11_Pv[1,])
Pv.ID <- matrix(wb11_Pv_main[1,])

for (i in 2:nrow(wb11_Pf)) {
  Pf.OD <- rbind(Pf.OD,matrix(wb11_Pf[i,]))
  Pf.ID <- rbind(Pf.ID,matrix(wb11_Pf_main[i,]))
  Pv.OD <- rbind(Pv.OD,matrix(wb11_Pv[i,]))
  Pv.ID <- rbind(Pv.ID,matrix(wb11_Pv_main[i,]))
}
#
t<-seq(1,12,1)
x <- c()
x0 <- 50
f <- function(x){x*2}
x[1]<-x0
for (i in 1:(length(t)-1)) {
  x[i+1] = f(x[i])
}
#

wb11_ALL <- data.frame(paste0("N",b[j]),
                      rbind(Pf.ID,Pv.ID),
                      rbind(matrix(rep("std",12)),
                            matrix(rep("unk",80)),"std",
                            matrix(rep("ctr",2)),"std"), 
                      rep(x[length(x)]/x), ##FACTOR for AB.UNITS
                      rbind(Pf.OD,Pv.OD), 
                      gl(2,96, labels = c("Pfal","Pviv")))
colnames(wb11_ALL) <- c("Plate", "ID", "Type", "Ab.unit", "OD", "Specie")
wb11_ALL$ID <- unlist(wb11_ALL$ID)
wb11_ALL$OD <- unlist(wb11_ALL$OD)
wb11_ALL$ID <- factor(wb11_ALL$ID)
# 2.1 AVOID NA's
wb11_ALL <- wb11_ALL[complete.cases(wb11_ALL[,2]),]
# 2.2 CLEAN data.frame
wb11_ALL[wb11_ALL$Type!="std","Ab.unit"] <- NA
wb11_ALL[wb11_ALL$ID=="Blank","Ab.unit"] <- 0
# 2.3 CORRECT variable class
wb11_ALL$OD <- as.numeric(wb11_ALL$OD)
#
# 2.4 IDENTIFY blank ctrl+ ctrl-
blank.Pfal <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="Blank" & wb11_ALL$Specie=="Pfal",5]))
blank.Pviv <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="Blank" & wb11_ALL$Specie=="Pviv",5]))
ctrPos.Pfal <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C+" & wb11_ALL$Specie=="Pfal",5]))
ctrPos.Pviv <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C+" & wb11_ALL$Specie=="Pviv",5]))
ctrNeg.Pfal <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C-" & wb11_ALL$Specie=="Pfal",5]))
ctrNeg.Pviv <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C-" & wb11_ALL$Specie=="Pviv",5]))
# 3 UNK MEAN OD
wb11_UNK <- subset(wb11_ALL, Type=="unk")
wb11_UNK$ID <- factor(wb11_UNK$ID) # unk goes from 
n <- length(levels(wb11_UNK$ID))
# 3.1 MEAN DATA.FRAME
wb11_MEAN <- data.frame(paste0("N",b[j]),
                        levels(wb11_UNK$ID),
                        rep("unk",length(levels(wb11_UNK$ID))), 
                        rep(NA,length(levels(wb11_UNK$ID))),#Ab.unit
                        rep(NA,length(levels(wb11_UNK$ID))),#mean.OD
                        gl(2,length(levels(wb11_UNK$ID)), 
                           labels = c("Pfal","Pviv")))
colnames(wb11_MEAN) <- c("Plate", "ID", "Type", "Ab.unit", "mean.OD", "Specie")
# 3.2 FEED MEAN DATA.FRAME
for (i in 1:n) {
  wb11_MEAN[i,5] <- mean(as.numeric(subset(wb11_UNK, 
                                           ID==levels(wb11_UNK$ID)[i] &
                                             Specie==levels(wb11_UNK$Specie)[1])$OD))
  wb11_MEAN[n+i,5] <- mean(as.numeric(subset(wb11_UNK, 
                                             ID==levels(wb11_UNK$ID)[i] &
                                               Specie==levels(wb11_UNK$Specie)[2])$OD))
}
#
######
###### [OFF] BACKGROUND SUBTRACTION
######
# 4 STD.DEV DATA.FRAME
wb11_SDCV <- cbind(wb11_MEAN, 
                   rep(NA,length(levels(wb11_UNK$ID))),#mean.OD
                   rep(NA,length(levels(wb11_UNK$ID))))#sd.OD)
colnames(wb11_SDCV) <- c("Plate", "ID", "Type", "Ab.unit", "mean.OD", "Specie", "sd.OD", "cv.OD")
# 4.1 FEED STD.DEV DATA.FRAME
for (i in 1:n) {
  wb11_SDCV[i,7] <- sd(subset(wb11_UNK, 
                              ID==levels(wb11_UNK$ID)[i] &
                                Specie==levels(wb11_UNK$Specie)[1])$OD)
  wb11_SDCV[n+i,7] <- sd(subset(wb11_UNK, 
                                ID==levels(wb11_UNK$ID)[i] &
                                  Specie==levels(wb11_UNK$Specie)[2])$OD)
}

wb11_SDCV[,8] <- 100*(wb11_SDCV[,7] / wb11_SDCV[,5])
#
# 5 PARAMETER ESTIMATION 4pLL model
#
wb11.m1 <- drm(OD ~ Ab.unit, Specie, 
               data= subset(wb11_ALL, Type=="std"),
               fct = LL.4(names = c("b", "c", "d", "e")))
#
wb11.model <- wb11.m1
# 6 BOX-COX TRANSFORMATION against RESIDUAL heterogeneity
wb11.model.BX <- boxcox(wb11.model, 
                     main=expression("Optimal " ~ lambda ~ " with confidence intervals"), 
                     plotit = FALSE)
# 7 UNK AB.UNITS ESTIMATION by INVERSE REGRESSION
wb11_Resp.Pf <- ED(wb11.model.BX, 
                   wb11_MEAN[1:n,5],
                   type = "absolute",interval = "delta",
                   clevel = "Pfal", display = FALSE)
wb11_Resp.Pv <- ED(wb11.model.BX, 
                   wb11_MEAN[(n+1):(2*n),5],
                   type = "absolute",interval = "delta",
                   clevel = "Pviv", display = FALSE)
# 7.1 FEED UNK AB.UNITS DATA.FRAME
for (i in 1:n) {
  wb11_SDCV[i,4] <- wb11_Resp.Pf[i]
  wb11_SDCV[n+i,4] <- wb11_Resp.Pv[i]
}
#
####
#### END ANALYSIS
#
#### START PLOTTING
####
#
par(mfrow=c(4,3))
### 1 DISTRIBUTION PLOTING
plot(OD ~ log(Ab.unit), 
     data=subset(wb11_ALL, Type=="std" & Specie=="Pfal"), 
     ylim = c(0,1.5), col="red",
     xlab = "log(Ab unit)", ylab = "OD 450nm",
     main= paste0("ELISA plate"," ","N",b[j],"\n Standard samples distribution"))
points(OD ~ log(Ab.unit), 
       data=subset(wb11_ALL, Type=="std" & Specie=="Pviv"), 
       pch=2, col="blue")
abline(h=c(ctrNeg.Pfal, ctrNeg.Pviv), lty=c(3, 3), col=c("red", "blue"))
abline(h=c(ctrPos.Pfal, ctrPos.Pviv), lty=c(3, 3), col=c("red", "blue"))
abline(h=c(blank.Pfal, blank.Pviv), lty=c(2, 2), col=c("red", "blue"))
legend("bottomright", c("P.vivax","P.falcip", "C+/-", "blank"), 
       col = c( "blue", "red", "black", "black"), 
       pch = c(2, 1, NA, NA), #lwd= c(1,1),
       lty = c(NA, NA, 3, 2),
       cex = 0.8,
       inset = .01#, merge = TRUE
)
#
### 2 %CV CONDITIONAL PLOTING
plot(cv.OD ~ mean.OD,wb11_SDCV, 
     ylim=c(0,100), xlim=c(0,1.5),
     xlab="mean OD 450nm of duplicates",
     ylab="%CV of duplicates",
     main="Percentage Coefficient of Variation")
abline(h=20, v=0.25, lty=2, col=2)
#
### 3 UNK AB.UNITS PLOTING
plot(wb11.model.BX,#broken = TRUE,
     ylim = c(0,1.5), 
     xlim = c(0,5e3),
     main=paste0("ELISA plate"," ","N",b[j],"\n Unknown samples distribution"),
     col="grey",
     legendPos = c(2,1.5))
points(y=wb11_MEAN[(n+1):(2*n),5],x=wb11_Resp.Pv[1:n],col="blue",pch=4)
points(y=wb11_MEAN[1:n,5],x=wb11_Resp.Pf[1:n],col="red",pch=4)
#abline(h=c(ctrNeg.Pfal, ctrNeg.Pviv), lty=c(3, 3), col=c("red", "blue"))
#abline(h=c(ctrPos.Pfal, ctrPos.Pviv), lty=c(3, 3), col=c("red", "blue"))
#abline(h=c(blank.Pfal, blank.Pviv), lty=c(2, 2), col=c("red", "blue"))
legend("bottomright", c("P.vivax","P.falcip"), #, "C+/-", "blank"
       col = c( "blue", "red"), #, "black", "black"
       pch = c(4, 4), #, NA, NA #lwd= c(1,1),
       #lty = c(NA, NA), #, 3, 2
       cex = 0.8,
       inset = .01#, merge = TRUE
)
#
### 4 VARIANCE HETEROGENEITY
plot(residuals(wb11.model) ~ fitted(wb11.model), 
     main="4p Log-Logistic model \n Variance Hetereogeneity",
     xlab="fitted",
     ylab = "residuals")
abline(h=0)
#
### 5 LAMBDA ESTIMATE ##------Box-Cox transformation-------##
wb11.model.BX <- boxcox(wb11.model, 
                     main=expression("Optimal " ~ lambda ~ " with confidence intervals"))
#
### 6 VARIANCE HOMOGENEITY
plot(residuals(wb11.model.BX) ~ fitted(wb11.model.BX), 
     main="Box-Cox transformed model \n Variance Homogeneity",
     xlab="fitted",
     ylab = "residuals")
abline(h=0)
#
### 7 RESIDUAL SKWENESS
qqnorm(residuals(wb11.model), 
       main = "4p Log-Logistic model \n Normal Q-Q Plot")
qqline(residuals(wb11.model))
#
### 8 DRC AND BOXCOX OVERLAPPED MODELS
plot(wb11.model, #broken = TRUE, 
     xlab="Ab units", ylab="OD 450nm",
     ylim = c(0,1.5), 
     xlim = c(0,5e3),
     main="in black: 4p Log-Logistic model \n in red: Box-Cox transformed model",
     legendPos = c(2,1.5))
plot(wb11.model.BX, col = "red", add = TRUE, legend = FALSE,
     ylim = c(0,1.5), 
     xlim = c(0,5e5))
#
### 9 RESIDUAL NORMALITY
qqnorm(residuals(wb11.model.BX), 
       main="Box-Cox transformed model \n Normal Q-Q Plot")
qqline(residuals(wb11.model.BX))
#
####
####
#
### 10 OD REPLICATES DISTRIBUTION
#par(mfrow=c(3,3))
#my <- max(hist(wb11_ALL$OD, plot = F)$counts, na.rm=TRUE)
#hist(wb11_ALL$OD, ylim = c(0,my), 
#     main= paste0("ELISA plate"," ","N",b[j]),
#     xlab = "OD")
plot(density(na.omit(wb11_ALL$OD)),
     main = "OD Density plot",
     xlab = "OD")#, ylim = c(0,my)  #, xlim = c(0,mx)
#qqnorm(wb11_ALL$OD); qqline(wb11_ALL$OD)
#
### 11 OD MEAN DISTRIBUTION
#hist(wb11_MEAN$mean.OD, ylim = c(0,my),
#     main = "mean.OD Histogram",
#     xlab = "mean.OD")
plot(density(na.omit(wb11_MEAN$mean.OD)),
     main = "mean.OD Density plot",
     xlab = "mean.OD")#, ylim = c(0,my)
#qqnorm(wb11_MEAN$mean.OD); qqline(wb11_MEAN$mean.OD)
#
### 12 AB.UNIT DISTRIBUTION
#hist(wb11_SDCV$Ab.unit, ylim = c(0,my),
#     main = "Ab.units Histogram",
#     xlab = "Ab.units")
plot(density(na.omit(wb11_SDCV$Ab.unit)),
     main = "Ab.units Density plot",
     xlab = "Ab.units")#, ylim = c(0,my)
#qqnorm(wb11_SDCV$Ab.unit); qqline(wb11_SDCV$Ab.unit)
#
####
#### END PLOTTING
#### START ROW BINDING TO FINAL DATA.FRAME
####
wb11_Ab <- rbind(wb11_Ab,wb11_SDCV)
####
paramALL <- rbind(paramALL, t(data.frame(coefficients(wb11.model.BX)[3:6])))
####
}

```

# Results
```{r}
# to FREEZE and ANALYSE
freeze <- wb11_Ab
#wb11_Ab <- freeze
```

## DB typo corrections

After showing the missing of **02** samples in the Templates, typos were identified and the corresponding modifications were made following Reproducible Reseach paradigms, i.e. avoiding any editing on the original DB.

This correction introduced the following samples:

- **ZG009-3**, by changing *ZG009-1* in plate **N17**, and

```{r}
#str(wb11_Ab)
wb11_Ab$ID <- as.character(wb11_Ab$ID)

wb11_Ab[wb11_Ab$ID=="ZG009-1",]
#wb11_Ab[wb11_Ab$ID=="ZG009-1" & wb11_Ab$Plate=="N17",]
wb11_Ab[wb11_Ab$ID=="ZG009-1" & wb11_Ab$Plate=="N17",2] <- rep("ZG009-3",2)
#wb11_Ab[wb11_Ab$ID=="ZG009-1",]
wb11_Ab[wb11_Ab$ID=="ZG009-3",]
```

- **NN068-2**, by changing *NN082-5* in plate **N33**.

```{r}
wb11_Ab[wb11_Ab$ID=="NN082-5",]
#wb11_Ab[wb11_Ab$ID=="NN082-5" & wb11_Ab$Plate=="N33",]
wb11_Ab[wb11_Ab$ID=="NN082-5" & wb11_Ab$Plate=="N33",2] <- rep("NN068-2",2)
wb11_Ab[wb11_Ab$ID=="NN068-2",]
#wb11_Ab[wb11_Ab$ID=="NN082-5",]

wb11_Ab$ID <- as.factor(wb11_Ab$ID)
#str(wb11_Ab)
```

As expected, this reduced the number of duplicates.

## ALL

```{r}
## total de ID evaluados
mue <- length(levels(as.factor(wb11_Ab$ID))) # TOTAL MUESTRAS EVALUADAS
## total ESPERADO + REPLICAS
rep <- dim(wb11_Ab)[1] # TOTAL INICIAL
## total ESPERADO de ID x2 SPECIES-REPLICATES (uno de P.VIVAX y uno de P.FALCIPARUM)
esp <- 2*length(levels(as.factor(wb11_Ab$ID))) # TOTAL ESPERADO
## total de ID REPETIDOS (corridos por duplicado o triplicado en nuevos PLATES) y que se deben retirar
repet <- rep - esp
```

The total amount of evaluated samples is **`r mue`**. However, due to the presence of replicates among Templates, the initial number of reported reads with both `mean.OD` and estimated `Ab.unit` goes up to **`r rep`**.

If each sample have 02 reads, 01 per specie, then the **total expected** amount of samples is **`r esp`**. The rest are replicates (**`r repet`**).

Here is the total amount of evaluated samples in both Pv/Pf plates of each Template file:

```{r}
## total de muestras por TEMPLATE (P.VIVAX igual a P.FALCIPARUM)
table(wb11_Ab[wb11_Ab$Specie=="Pviv",]$Plate)
```

- 04 Templates with lower samples than expected:
    + N12 (n=39), N18 (n=36), N23 (n=10), N41 (n=18).

- 01 Template with more samples than expected:
    + N15 (n=41)
        - **ZG181-1** sampled against **Pviv** plate only
        - **ZG182-1** sampled against **Pfal** plate only.
    + Both generated NaNs at the end.
    + Further dicussion at the end of the next section.

## NA

The total amount of samples with **unestimated `Ab.unit` goes up to `r sum(is.na(wb11_Ab$Ab.unit))`**:
```{r}
## muestras con Ab.units NO estimadas
table(wb11_Ab[is.na(wb11_Ab$Ab.unit)==TRUE,]$Plate)

## MOTIVO: above plate-specific model upper limit or below its lower limit
#sum(is.na(wb11_Ab$Ab.unit))
naMAT <- wb11_Ab[is.na(wb11_Ab$Ab.unit)==TRUE,]
#naMAT
```

**NOTE:** Only **03 samples** with NaN in both reads. Sample **ZG177-4** will be further evaluated in the `Duplicates` section of [UNBIASED selection](#unbiased-selection):
```{r}
NaN_2 <- names(which(table(factor(wb11_Ab[is.na(wb11_Ab$Ab.unit)==TRUE,]$ID))==2))
NaN_2

NaN_2_MAT <- data.frame()
for (i in 1:length(NaN_2)) {
  NaN_2_MAT <- rbind(NaN_2_MAT,wb11_Ab[wb11_Ab$ID==NaN_2[i],])
}

#NaN_2_MAT[-c(7:10),]
NaN_2_MAT[NaN_2_MAT$ID!="ZG177-4",]

```

On the other hand, the total amount of samples with **`mean.OD` with `NA` is `r sum(is.na(wb11_Ab$mean.OD))`**:
```{r}
## muestras con OD con valor NA?
#sum(is.na(wb11_Ab$mean.OD))
wb11_Ab[is.na(wb11_Ab$mean.OD)==TRUE,]
```

Both samples belong to **N15**. The problem here was that in this Template, one sample was evaluates in the Pf plate, and the other one in the Pv plate: 

- **ZG181-1**, since it is a replicate, it have reads for both species.
```{r}
wb11_Ab[wb11_Ab$ID=="ZG181-1",]
```

- However, **ZG182-1** was only evaluated once, for one ELISA plate and for one specie.
```{r}
wb11_Ab[wb11_Ab$ID=="ZG182-1",]
```

## REPLICATES

From the **total expected** samples, we calculate that the total amount of reads in excess that came from replicates (duplicates or triplicates) among Templates goes up to **`r repet`**.

```{r}
## FRECUENCIA por ID
Ab_freq <- data.frame(table(as.factor(wb11_Ab$ID)))

## total de ID con mas de una replica por especie
#dim(Ab_freq[Ab_freq$Freq>2,])[1]

## ID con duplicado
#dim(Ab_freq[Ab_freq$Freq==4,])[1]

## ID con triplicado?
#dim(Ab_freq[Ab_freq$Freq==6,])[1]

## RESULTADO DE SUBTRACCION DE REPLICAS
## ID con duplicado + ## ID con triplicado
subtra <- 2*(sum(Ab_freq$Freq==4)) + 4*(sum(Ab_freq$Freq==6)) # cantidades a subtraer
#subtra

## LA SUBTRACCION DE REPLICAS es IGUAL al total de ID A RETIRAR
#subtra == repet # 194
```

The total amount of samples with **duplicated** reads is **`r dim(Ab_freq[Ab_freq$Freq==4,])[1]`** and with **triplicated** reads is **`r dim(Ab_freq[Ab_freq$Freq==6,])[1]`**.

```{r}
summary(factor(Ab_freq$Freq))
```

### Triplicates

Listado de muestras **triplicadas** (n=`r dim(Ab_freq[Ab_freq$Freq==6,])[1]`):
```{r}
## LISTADO de muestras TRIPLICADAS
triplOD <- Ab_freq[Ab_freq$Freq==6,]$Var1

triplMAT <- data.frame()
for (i in 1:length(triplOD)) {
  triplMAT <- rbind(triplMAT,wb11_Ab[wb11_Ab$ID==triplOD[i],])
}

as.character(triplOD)

dim(triplMAT)
#head(triplMAT,12)
summary(triplMAT[,-c(3,6)])
```

### Duplicates

Listado de muestras **duplicadas** (n=`r dim(Ab_freq[Ab_freq$Freq==4,])[1]`):
```{r}
## LISTADO de muestras DUPLICADAS
dupliOD <- Ab_freq[Ab_freq$Freq==4,]$Var1

dupliMAT <- data.frame()
for (i in 1:length(dupliOD)) {
  dupliMAT <- rbind(dupliMAT,wb11_Ab[wb11_Ab$ID==dupliOD[i],])
}

as.character(dupliOD)

dim(dupliMAT)
#head(dupliMAT, 12)
summary(dupliMAT[,-c(3,6)])
```

## UNBIASED selection

Unbiased selection criteria of Ab.units estimate among replicated reads of per sample:

+ **NOTE:** This assumes that no dilution to the original evaluated sample has been performed.
    + For triplicates, the closest value to the mean among replicates was selected
    + Reads with mean.OD with a %CV higher than 20% were rejected
    + For duplicates, the replicate with lower %CV was preferred.
    + If a selection was not acomplished, the last read performed was preferred.

This criteria have been implemented in a **function** called `Unbiased()`:

```{r}
#### FUNCION PARA LA APLICACION DEL CRITERIO
Unbiased <- function(repliMAT) {
  # LOOP: closest value to the mean
  z <- NULL
  # for each ID with replicates
  for(i in 1:length(levels(repliMAT$ID))){
    # for each Specie
    for (j in 1:length(levels(repliMAT$Specie))) {
      # extrae Ab.unit y %CV para cada especie
      x <- repliMAT[repliMAT$ID==levels(repliMAT$ID)[i] & 
                    repliMAT$Specie==levels(repliMAT$Specie)[j], c(4,8)]
      if (any(x$cv.OD>=20, na.rm = TRUE)) {
        # lecturas con %CV>=20% seran descartadas (independiente de OD>0.25)
        x <- x[-which(x$cv.OD>=20),]
      }
      y <- mean(x$Ab.unit, na.rm = TRUE)
      # elije cual de los valores es igual a la menor diferencia con la media aritmetica
      a <- x[which(min(abs(x$Ab.unit - y), na.rm = TRUE)==abs(x$Ab.unit - y)),]
    
      if (dim(a)[1]>1) {
        # para 2 lecturas, elegir la que tenga menor %CV
        a <- a[which(a$cv.OD==min(a$cv.OD, na.rm = TRUE)),]$Ab.unit
        if (length(a)>1) {
          # si no hay diferencias por %CV, entonces optar por la ultima lectura
          a <- a[2] 
        }
      } else {
        # si la dimension es igual a 1, entonces extraer las Ab.units
        a <- a$Ab.unit
      }
    
      z <- c(z,a)
    }
  }
  # LOOP para asignar eleccion a una tabla
  w <- NULL
  for(i in 1:length(z)){
    # evita IDENTIDAD entre Ab.Units de distintas muestras
    # seleccion por bloques
    v <- repliMAT[repliMAT$ID==levels(repliMAT$ID)[ceiling(i/2)],] 
    w <- rbind(w,subset(v, Ab.unit==z[i])) 
  }
  # output
  return(w)
}
####
```


The results will be shown as follows:

- First, the number of reads per samples: equal to 06 for triplicates and 04 for duplicates. 

- After criteria execution, the same table is presented with only 02 remaining reads per sample, 01 per specie.

- Finally, a table is presented with the samples and selected reads.

### Triplicates
```{r}
triplMAT$ID <- factor(triplMAT$ID)
table(triplMAT$ID)
#triplMAT[triplMAT$ID==levels(triplMAT$ID)[1],]

w <- Unbiased(triplMAT)
# output
table(w$ID)
dim(w)
summary(w[,-c(3,6)])
#w

triplNEW <- w

## CHECK
#length(z) == dim(w)[1]
#sum(table(w$ID)!=2)
```

### Duplicates

Sample **ZG177-4** is initially removed due to both reads for **Pfal** with **NaN**. After criteria execution, the selected replicated read for **Pviv** is going to be added.

```{r}
dupliMAT$ID <- factor(dupliMAT$ID)
#table(dupliMAT$ID)
extra <- dupliMAT[dupliMAT$ID=="ZG177-4",]
extra
dupliMAT <- dupliMAT[dupliMAT$ID!="ZG177-4",] # RETIRAR debido a que tiene DOS lecturas con NAN para FALCIPARUM
dupliMAT$ID <- factor(dupliMAT$ID)
table(dupliMAT$ID)
#dupliMAT[dupliMAT$ID==levels(dupliMAT$ID)[1],]

w <- Unbiased(dupliMAT)
# output
table(w$ID)
dim(w)
summary(w[,-c(3,6)])
#w
dupliNEW <- w

## CHECK:
#length(z) == dim(w)[1]
#sum(table(w$ID)!=2)
#

# ADD EXTRA "ZG177-4" only VIVAX
dupliNEW <- rbind(dupliNEW,extra[4,]) # +1
#dim(wb11_Ab)

```

## FILTERING

- Aim:
    + Retrieve of all replicated reads: triplicates, duplicates and NaN.
    + Addition of selected reads among triplicates and duplicates.
    + Visualization of mean.OD-Ab.unit non-linearity and whole %CV before and after the procedure

### Plot previous to filtering

```{r, fig.align='center', fig.height=3, fig.width=9}
summary(wb11_Ab[,-c(1:2)])
par(mfrow=c(1,3))
### NON LINEAR RELATIONSHIP
plot(mean.OD ~ Ab.unit,wb11_Ab,
     ylim = c(0,1.5), 
     xlim = c(0,5e3),
     main="OD-Ab.unit Non-linearity", pch=4) 
with(subset(wb11_Ab, Specie=="Pviv"), 
     points(Ab.unit, mean.OD, col="blue", pch=4))
with(subset(wb11_Ab, Specie=="Pfal"), 
     points(Ab.unit, mean.OD, col="red", pch=4))
legend("topright", c("P.vivax","P.falcip"), #, "C+/-", "blank"
       col = c( "blue", "red"), #, "black", "black"
       pch = c(4, 4), #, NA, NA #lwd= c(1,1),
       #lty = c(NA, NA), #, 3, 2
       cex = 0.8,
       inset = .01#, merge = TRUE
)
plot(mean.OD ~ log(Ab.unit),wb11_Ab,
     ylim = c(0,1.5), 
     xlim = c(-6,10),
     main="OD-Ab.unit Non-linearity", pch=4) 
with(subset(wb11_Ab, Specie=="Pviv"), 
     points(log(Ab.unit), mean.OD, col="blue", pch=4))
with(subset(wb11_Ab, Specie=="Pfal"), 
     points(log(Ab.unit), mean.OD, col="red", pch=4))
legend("topleft", c("P.vivax","P.falcip"), #, "C+/-", "blank"
       col = c( "blue", "red"), #, "black", "black"
       pch = c(4, 4), #, NA, NA #lwd= c(1,1),
       #lty = c(NA, NA), #, 3, 2
       cex = 0.8,
       inset = .01#, merge = TRUE
)
### %CV
plot(cv.OD ~ mean.OD,wb11_Ab, 
     ylim=c(0,100), xlim=c(0,1.5),
     xlab="mean OD 450nm of duplicates",
     ylab="%CV of duplicates",
     main="Percentage Coefficient of Variation")
abline(h=20, v=0.25, lty=2, col=2)
## SD
#plot(sd.OD ~ mean.OD,wb11_Ab,
#     ylim = c(0,0.27),
#     main="Heteroskedasticity?") 
```

### Data filtering

```{r}
#mue # TOTAL MUESTRAS EVALUADAS
#rep # TOTAL INICIAL
#esp # TOTAL ESPERADO

##
# to FREEZE and ANALYSE
freeze <- wb11_Ab
#wb11_Ab <- freeze
##
```

**`r mue` is the total amount of evaluated samples**. For each one, 02 read were made: 01 per specie. Before any filtering, the total amount of reads goes up to **`r rep` including replicates**. After filtering is applied, the **expected amount of left reads is `r esp`**.

A **function** called `checkIN()` would allow to evaluate the retrieving of replicated reads and addition of selected ones:

```{r}
#CHECK FUNCTION
checkIN <- function(a) {
  evalREP <- NULL
  for (i in 1:6) {
    evalREP <- c(evalREP, sum(table(a$ID)==i))
  }
  evalREP
}
```

First, replicates are retrieve:
```{r, eval=TRUE}
## RETRIEVE
dim(wb11_Ab)[1]
checkIN(wb11_Ab)
# triplicates
for (i in 1:length(as.character(triplOD))) {
  wb11_Ab <- wb11_Ab[wb11_Ab$ID!=as.character(triplOD)[i],] # -96
}
checkIN(wb11_Ab)
#dim(wb11_Ab)[1]
# duplicates
for (i in 1:length(as.character(dupliOD))) {
  wb11_Ab <- wb11_Ab[wb11_Ab$ID!=as.character(dupliOD)[i],] # -256
}
checkIN(wb11_Ab)
#dim(wb11_Ab)[1]
# NA
wb11_Ab <- wb11_Ab[is.na(wb11_Ab$Ab.unit)!=TRUE,] # -73
checkIN(wb11_Ab)
#
#dim(wb11_Ab)[1]
#
# 3 muestras que pierden Pviv y Pfal por NaN en ambos Ab.unit
#((1772 - 1718) - 48)/2 == (length(NaN_2) -1)
##
```

Then, selected reads added:
```{r}
## ADDITION
# ADD NEW TRIPLICATES
wb11_Ab <- rbind(wb11_Ab,triplNEW) # + 32 = 16x2
checkIN(wb11_Ab)
#dim(wb11_Ab)
# ADD NEW DUPLICATES
wb11_Ab <- rbind(wb11_Ab,dupliNEW) # +128 + 1 = 64x2 + 1
checkIN(wb11_Ab)
dim(wb11_Ab)[1]
```

### Filtering evaluation

```{r}
#mue
# FINAL
#dim(wb11_Ab)[1]
#esp - dim(wb11_Ab)[1] # 55 ?

# CLEAN FINAL DATA.FRAME
wb11_Ab$ID <- factor(wb11_Ab$ID)
#length(levels(wb11_Ab$ID))

#mue - length(levels(wb11_Ab$ID)) == (length(NaN_2) -1) # 3 muestras que pierden Pviv y Pfal por NaN Ab.unit

#length(levels(wb11_Ab$ID)) + (esp - dim(wb11_Ab)[1])
```

After filtering, we obtain **`r length(levels(wb11_Ab$ID))` samples** of **`r mue` expected**, giving a **total of `r dim(wb11_Ab)[1]` reads** on the data frame.

As seen in the last `checkIN`, **`r checkIN(wb11_Ab)[1]` samples** have only one read because of **Ab.unit NaN**. For more detail, go to [Appendix C](#appendix-c-residual-na)

In addition, **`r (length(NaN_2) -1)` samples** were automatically retrieved as a consequence of **Ab.unit NaN** in both species reads. Look back to [NA description](#na).

```{r}
#CHECK
#sum(table(wb11_Ab$ID)!=2) # 49 muestras que pierden a su par por NaN en Ab.units
#sum(table(wb11_Ab$ID)!=2) + (length(NaN_2) -1)*2 == esp - dim(wb11_Ab)[1] # 55 ?

#names(which(table(wb11_Ab$ID)!=2))
```

### Plot after filtering
```{r, fig.align='center', fig.height=3, fig.width=9}
summary(wb11_Ab[,-c(1:2)])
par(mfrow=c(1,3))
### NON LINEAR RELATIONSHIP
plot(mean.OD ~ Ab.unit,wb11_Ab,
     ylim = c(0,1.5), 
     xlim = c(0,5e3),
     main="OD-Ab.unit Non-linearity", pch=4) 
with(subset(wb11_Ab, Specie=="Pviv"), 
     points(Ab.unit, mean.OD, col="blue", pch=4))
with(subset(wb11_Ab, Specie=="Pfal"), 
     points(Ab.unit, mean.OD, col="red", pch=4))
legend("topright", c("P.vivax","P.falcip"), #, "C+/-", "blank"
       col = c( "blue", "red"), #, "black", "black"
       pch = c(4, 4), #, NA, NA #lwd= c(1,1),
       #lty = c(NA, NA), #, 3, 2
       cex = 0.8,
       inset = .01#, merge = TRUE
)
plot(mean.OD ~ log(Ab.unit),wb11_Ab,
     ylim = c(0,1.5), 
     xlim = c(-6,10),
     main="OD-Ab.unit Non-linearity", pch=4) 
with(subset(wb11_Ab, Specie=="Pviv"), 
     points(log(Ab.unit), mean.OD, col="blue", pch=4))
with(subset(wb11_Ab, Specie=="Pfal"), 
     points(log(Ab.unit), mean.OD, col="red", pch=4))
legend("topleft", c("P.vivax","P.falcip"), #, "C+/-", "blank"
       col = c( "blue", "red"), #, "black", "black"
       pch = c(4, 4), #, NA, NA #lwd= c(1,1),
       #lty = c(NA, NA), #, 3, 2
       cex = 0.8,
       inset = .01#, merge = TRUE
)
### %CV
plot(cv.OD ~ mean.OD,wb11_Ab, 
     ylim=c(0,100), xlim=c(0,1.5),
     xlab="mean OD 450nm of duplicates",
     ylab="%CV of duplicates",
     main="Percentage Coefficient of Variation")
abline(h=20, v=0.25, lty=2, col=2)
## SD
#plot(sd.OD ~ mean.OD,wb11_Ab,
#     ylim = c(0,0.27),
#     main="Heteroskedasticity?") 
```

## EXPORT

- Aim:
    + 01 row, 01 sample, 02 reads

```{r}
final <- wb11_Ab
#summary(final[,-c(1:2)])

finalDB <- data.frame()
for (i in 1:length(levels(final$ID))) {
  finMAT <- data.frame(ID= levels(final$ID)[i], 
                       Ab.unit.Pfal= 
                         if (identical(final[final$ID==levels(final$ID)[i] & 
                                             final$Specie==levels(final$Specie)[1],4],
                                       numeric(0))) {
                           NaN
                         } else {
                           final[final$ID==levels(final$ID)[i] & 
                                   final$Specie==levels(final$Specie)[1],4]
                         }
                       , 
                       Ab.unit.Pviv= 
                         if (identical(final[final$ID==levels(final$ID)[i] & 
                                             final$Specie==levels(final$Specie)[2],4],
                                       numeric(0))) {
                           NaN
                         } else {
                           final[final$ID==levels(final$ID)[i] & 
                                   final$Specie==levels(final$Specie)[2],4]
                         }
                       )
  finalDB <- rbind(finalDB,finMAT)
}
#colnames(finalDB) <- c("ID", "Ab.unit.Pfal", "Ab.unit.Pviv")
summary(finalDB)
dim(finalDB)
```

```{r}
## ADDITION of 03 samples with NaN in both reads --> THIS completes the 967 final samples
#tail(finalDB)
#NaN_2[-1]

## IF NaN changed by ZERO, then the other 03 samples with OD above upper limit should have an input Ab.unit
# go to "re-run"
```

The `finalDB` output is write as `Standardized_ELISA_ALL.csv`:
```{r}
finalDBx <- format(finalDB, scientific=F, digits=2)

# WRITE the final CSV
write.csv(finalDBx, "data/Standardized_ELISA_ALL.csv")
```


# OD vs Ab distribution

## OD distribution
```{r, fig.align='center', fig.height=9, fig.width=9}
par(mfrow=c(3,3))

my <- max(hist(wb11_Ab$mean.OD, plot = F)$counts, na.rm=TRUE)

hist(wb11_Ab$mean.OD, xlim = c(0,1.4), 
     main = "mean.OD Histogram", xlab = "mean.OD")
plot(density(na.omit(wb11_Ab$mean.OD)), 
     main = "mean.OD Density Plot", xlab = "mean.OD")#, ylim = c(0,my)
qqnorm(wb11_Ab$mean.OD); qqline(wb11_Ab$mean.OD)

hist(wb11_Ab[wb11_Ab$Specie=="Pviv",]$mean.OD, ylim = c(0,my), xlim = c(0,1.4), 
     main = "P.vivax", xlab = "mean.OD")
plot(density(na.omit(wb11_Ab[wb11_Ab$Specie=="Pviv",]$mean.OD)),
     main = "P.vivax", xlab = "mean.OD")#, ylim = c(0,my)
qqnorm(wb11_Ab[wb11_Ab$Specie=="Pviv",]$mean.OD); qqline(wb11_Ab[wb11_Ab$Specie=="Pviv",]$mean.OD)

hist(wb11_Ab[wb11_Ab$Specie=="Pfal",]$mean.OD, ylim = c(0,my), xlim = c(0,1.4),
     main = "P.falciparum", xlab = "mean.OD")
plot(density(na.omit(wb11_Ab[wb11_Ab$Specie=="Pfal",]$mean.OD)),
     main = "P.falciparum", xlab = "mean.OD")#, ylim = c(0,my)
qqnorm(wb11_Ab[wb11_Ab$Specie=="Pfal",]$mean.OD); qqline(wb11_Ab[wb11_Ab$Specie=="Pfal",]$mean.OD)
```

## Ab distribution

```{r, fig.align='center', fig.height=9, fig.width=9}
par(mfrow=c(3,3))

my <- max(hist(wb11_Ab$Ab.unit, plot = F)$counts, na.rm=TRUE)

hist(wb11_Ab$Ab.unit, 
     main = "Ab.units Histogram", xlab = "Ab.units")#, xlim = c(0,15000)
plot(density(na.omit(wb11_Ab$Ab.unit)), 
     main = "Ab.units Density Plot", xlab = "Ab.units")#, ylim = c(0,my)
qqnorm(wb11_Ab$Ab.unit); qqline(wb11_Ab$Ab.unit)

hist(wb11_Ab[wb11_Ab$Specie=="Pviv",]$Ab.unit, ylim = c(0,my),
     main = "P.vivax", xlab = "Ab.units")#, xlim = c(0,15000)
plot(density(na.omit(wb11_Ab[wb11_Ab$Specie=="Pviv",]$Ab.unit)),
     main = "P.vivax", xlab = "Ab.units")#, ylim = c(0,my)
qqnorm(wb11_Ab[wb11_Ab$Specie=="Pviv",]$Ab.unit); qqline(wb11_Ab[wb11_Ab$Specie=="Pviv",]$Ab.unit)

hist(wb11_Ab[wb11_Ab$Specie=="Pfal",]$Ab.unit, ylim = c(0,my),
     main = "P.falciparum", xlab = "Ab.units")#, xlim = c(0,15000)
plot(density(na.omit(wb11_Ab[wb11_Ab$Specie=="Pfal",]$Ab.unit)),
     main = "P.falciparum", xlab = "Ab.units")#, ylim = c(0,my)
qqnorm(wb11_Ab[wb11_Ab$Specie=="Pfal",]$Ab.unit); qqline(wb11_Ab[wb11_Ab$Specie=="Pfal",]$Ab.unit)
```

## Log scale

```{r, fig.align='center', fig.height=3, fig.width=9}
par(mfrow=c(1,3))

my <- max(hist(log(wb11_Ab$mean.OD), plot = F)$counts, na.rm=TRUE)

hist(log(wb11_Ab$mean.OD), xlim = c(-3.5,1),
     main = "mean.OD Histogram", xlab = "log(mean.OD)")
hist(log(wb11_Ab[wb11_Ab$Specie=="Pviv",]$mean.OD), ylim = c(0,my), xlim = c(-3.5,1),
     main = "P.vivax", xlab = "log(mean.OD)")
hist(log(wb11_Ab[wb11_Ab$Specie=="Pfal",]$mean.OD), ylim = c(0,my), xlim = c(-3.5,1),
     main = "P.falciparum", xlab = "log(mean.OD)")
```

```{r, fig.align='center', fig.height=3, fig.width=9}
par(mfrow=c(1,3))

my <- max(hist(log(wb11_Ab$Ab.unit), plot = F)$counts, na.rm=TRUE)

hist(log(wb11_Ab$Ab.unit), xlim = c(-6,10),
     main = "Ab.units Histogram", xlab = "Ab.units")
hist(log(wb11_Ab[wb11_Ab$Specie=="Pviv",]$Ab.unit), ylim = c(0,my), xlim = c(-6,10),
     main = "P.vivax", xlab = "log(Ab.units)")
hist(log(wb11_Ab[wb11_Ab$Specie=="Pfal",]$Ab.unit), ylim = c(0,my), xlim = c(-6,10),
     main = "P.falciparum", xlab = "log(Ab.units)")
```

```{r, fig.align='center', fig.width=8, fig.height=3, eval=FALSE}
#library(ggplot2)
#library(Rmisc)        #multiploting ggplots
a <- ggplot(wb11_Ab, aes(x=mean.OD, fill=Specie)) + 
  geom_histogram(alpha=.5, position = "identity") + 
  labs(title="Histograma de mean.OD") +
  scale_x_continuous(breaks = c(0,0.1,0.2,0.5,1,1.5),limits = c(0,1.5),
                     trans = "log1p", 
                     expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0),limits = c(0,600))
b <- ggplot(wb11_Ab, aes(x=Ab.unit, fill=Specie)) + 
  geom_histogram(alpha=.5, position = "identity") + 
  labs(title="Histograma de Ab.units") +
  scale_x_continuous(breaks = c(0,1,10,100,1000,10000,20000), 
                     trans = "log1p", 
                     expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0),limits = c(0,600))
multiplot(a,b, cols = 2)
```

# Covariates

## NEW DB comparison

A *bash* script line to copy the original covariate file:
```{bash}
cp data-raw/RESULTADOS_LABORATORIO_PRIMER\ BARRIDO\ \(JUL-\ AGO\ 2015\).xlsx data/Zungarococha_database2_16ene2017.xlsx
```

```{r}
pos <- loadWorkbook("data/Zungarococha_database2_16ene2017.xlsx")
#
post <- readWorksheet(pos, 
                      sheet = 1, 
                      startRow = 1, endRow = 961,
                      startCol = 1, endCol = 15)
#dim(post)
#head(post)

colnames(post) <- c("id", "age", "sex", "community", 
                    "micro1", "dens1", "micro2", "dens2", 
                    "pv200msp1", "pvmsp1", "pfmsp1", 
                    "pcrrubio_Pv", "pcrrubio_Pf", "pcrsnounou_Pv", "pcrsnounou_Pf")

#post=="."
post[post=="."] <- NA ## all dots changed to NA

post$pcrrubio <- post$pcrrubio_Pf + post$pcrrubio_Pv
post$pcrsnounou <- as.numeric(post$pcrsnounou_Pf) + as.numeric(post$pcrsnounou_Pv)

post$id <- as.factor(post$id)
post$sex <- as.factor(post$sex)
post$community <- as.factor(post$community)
post$micro1 <- as.factor(post$micro1)
post$dens1 <- as.numeric(post$dens1)
post$micro2 <- as.factor(post$micro2)
post$dens2 <- as.numeric(post$dens2)
post$pcrrubio <- as.factor(post$pcrrubio)
post$pcrsnounou <- as.factor(post$pcrsnounou)

post <- post[,-c(9:15)] ## excluding mean.OD

#str(post)
#summary(post)
```

```{r}
intCOMM<- intersect(as.character(finalDB$ID),as.character(post$id)) # COMMON
finalEXCL<- setdiff(as.character(finalDB$ID),as.character(post$id)) # FINAL EXCLUSIVE
postEXCL<- setdiff(as.character(post$id),as.character(finalDB$ID)) # POST EXCLUSIVE

#length(intCOMM) #955
#length(finalEXCL) #9
#length(postEXCL) #5

#NaN_2_MAT[-c(1:4),]
```

From the total of **`r mue`** evaluated samples, **03** were rejected due to Ab.units approaching zero in both reads, giving **`r length(finalDB$ID)`** available samples in the `finalDB`.

A comparison between the `finalDB` and the **updated** `Zungarococha_database2_16ene2017.xlsx`, showed that:

+ `Zungarococha...` have only **`r length(post$id)` samples**
+ both DB have **`r length(intCOMM)`** sample ID in common. 
+ `finalDB` have **`r length(finalEXCL)`** exclusive samples
```{r}
finalEXCL
```
These samples were arbitrarily rejected based in an **unknowkn criteria** yet. We must add more detail on this step.

+ `Zungarococha...` have **`r length(postEXCL)`** exclusive ones.
```{r}
postEXCL
```
This are the **3** automatically rejected samples, previously mentioned in [NA description](#na).


## MERGE Ab.units and covariates

- AIM:
    + First, retrieve the **`r length(finalEXCL)` excluded samples** [under which criteria?] due to unavailable covariate data.
    + Second, introduce the **`r length(postEXCL)` automatically rejected samples** [check [NA description](#na)]
    + Third, merge **`r length(post$id)` sample** Ab.units with its respective covariates.
    + Finally, write a CSV of the outcome
    
```{r}
chach <- finalDB
#finalDB <- chach

#dim(finalDB) # 966
## RETRIEVE excluded SAMPLES
for(i in 1:length(finalEXCL)) {
  finalDB <- finalDB[finalDB$ID!=finalEXCL[i],]
}
#dim(finalDB) #-9
```
    
```{r}
## ADD 03 automatically rejected samples as NaN (mean.OD below lower.limit)
#head(finalDB)
#tail(finalDB)

finalDB <- rbind(finalDB, 
                 data.frame(ID= postEXCL,
                            Ab.unit.Pfal= NaN,
                            Ab.unit.Pviv= NaN))
finalDB$ID <- factor(finalDB$ID)
#dim(finalDB) # +3
#tail(finalDB)
```

```{r}
### MERGING
#dim(post)
rownames(finalDB) <- NULL
rownames(post) <- NULL

finalDBcov <- data.frame()
for(i in 1:length(post$id)) {
  finalDBcov <- rbind(finalDBcov,
                      cbind(post[post$id==post$id[i],],
                            finalDB[finalDB$ID==post$id[i],]))
}
#finalDBcov$id == finalDBcov$ID
#dim(finalDBcov)
```

The `finalDBcov` output is write as `Standardized_ELISA_ALL_COVARIATES.csv`:
```{r}
str(finalDBcov)
summary(finalDBcov)
write.csv(format(finalDBcov[,-11], scientific=FALSE, digits=2), "data/Standardized_ELISA_ALL_COVARIATES.csv")# WITHOUT LABELING!!
```

The `finalDBcov` data.frame is ready to re-run the seropositivity analysis.

### EVALUATE 01 patient 02 IDs

- AIM:
    + Contrast the data of the samples mentioned in the 2nd sheet of `Zungarococha_database2...`

```{r}
deldobl <- readWorksheet(pos, 
                      sheet = 2, 
                      startRow = 20, endRow = 27,
                      startCol = 2, endCol = 3)
deldobl

#str(deldobl)
```

Strings were split into individual characters in the following way:
```{r}
strsplit(deldobl[5,], split = " ")[[1]]
```

Check equality between pair of samples:
```{r}
deldoblMIX <- data.frame()
for(i in 1:dim(deldobl)[1]) {
  deldoblMIX <- rbind(deldoblMIX,
                      finalDBcov[finalDBcov$id==
                                   strsplit(deldobl[i,], 
                                            split = " ")[[1]][1],],
                      finalDBcov[finalDBcov$id==
                                   strsplit(deldobl[i,], 
                                            split = " ")[[1]][3],])
}

format(deldoblMIX[,-11], scientific=FALSE, digits=2)#

#finalDBcov[finalDBcov$id==strsplit(deldobl[4,], split = " ")[[1]][1],]
```

Some equalities do not seem to be from the same patient. This step needs more support prior to any sample deletion.

## COMPARISON of covariates
```{r}
prev <- read.csv("data-raw/Zungarococha_database1_3Oct2016.csv")
#length(prev$id)
#length(finalDB$ID)
```

### PRE DB stats
```{r, fig.align='center', fig.height=3, fig.width=12}
par(mfrow=c(1,4))
hist(prev[prev$community==levels(prev$community)[1],]$age,
     ylim=c(0,150),
     xlab = "age",
     main=levels(prev$community)[1])
hist(prev[prev$community==levels(prev$community)[2],]$age,
     ylim=c(0,150),
     xlab = "age",
     main=levels(prev$community)[2])
hist(prev[prev$community==levels(prev$community)[3],]$age,
     ylim=c(0,150),
     xlab = "age",
     main=levels(prev$community)[3])
hist(prev[prev$community==levels(prev$community)[4],]$age,
     ylim=c(0,150),
     xlab = "age",
     main=levels(prev$community)[4])
```

```{r}
str(prev)
summary(prev)
```

### NEW DB stats

Covariate labeling is based on labels available in covariate DB sheet 2. If more clarity is required, we only need to change those cells.

```{r}
covLabl <- readWorksheet(pos, 
                      sheet = 2, 
                      startRow = 7, endRow = 16,
                      startCol = 2, endCol = 6)
#covLabl
#as.character(covLabl[1,c(2:3)])#sex
#covLabl[3,]#community
#covLabl[5,]#specie

levels(post$sex) <- as.character(covLabl[1,c(2:3)])#sex
levels(post$community) <- as.character(covLabl[3,c(2:5)])#sex
levels(post$micro1) <- as.character(covLabl[5,c(4,2,3,5)])#specie ##LOOK change in ORDER
levels(post$micro2) <- as.character(covLabl[5,c(4,2,3,5)])#specie ##LOOK change in ORDER
levels(post$pcrrubio) <- as.character(covLabl[5,c(4,2,3)])#specie 
levels(post$pcrsnounou) <- as.character(covLabl[5,c(4,2,3)])#specie
```

```{r, fig.align='center', fig.height=3, fig.width=12}
par(mfrow=c(1,4))
hist(post[post$community==levels(post$community)[1],]$age,
     ylim=c(0,150),
     xlab = "age",
     main=levels(post$community)[1])
hist(post[post$community==levels(post$community)[2],]$age,
     ylim=c(0,150),
     xlab = "age",
     main=levels(post$community)[2])
hist(post[post$community==levels(post$community)[3],]$age,
     ylim=c(0,150),
     xlab = "age",
     main=levels(post$community)[3])
hist(post[post$community==levels(post$community)[4],]$age,
     ylim=c(0,150),
     xlab = "age",
     main=levels(post$community)[4])
```

```{r}
str(post)
summary(post)
```

***

# APPENDIX A: From OD to Ab units

- Sepulveda et.al. [@Seplveda2015] cite two main publications:
    1. **Cook et.al., 2011 in Bioko Island, Equatorial Guinea** [@Cook2011], and
    2. **Cunha et.al., 2014 in Jacareacanga, Brazil** [@Cunha2014].
- Both papers followed the same experimental procedure to standardize the data across plates. Althought, non of them cite **Miura et.al., 2008**[@Miura2008].
- Nuno Sepulveda is co-author of the second paper [@Cunha2014].


## NAMRU-6 method

```{r}
t<-seq(1,12,1)
x <- c()
x0 <- 50
f <- function(x){x*2}
x[1]<-x0
for (i in 1:(length(t)-1)) {
  x[i+1] = f(x[i])
}
```


The method provided by Julio followed this procedure, making two main assumptions:

1. An **endpoint titer** equal to the last dilution of the STD, in this case `r format(x[length(x)], scientific=FALSE)`.
2. An equal **endpoint titer** among *P.falciparum* and *P.vivax* STD's.

```{r}
example <- cbind(unlist(Pf.ID[1:12,]), 
                 x, 
                 gsub('STD 1','102400',unlist(Pf.ID[1:12,])),
                 x[length(x)]/x)
colnames(example) <- c("Dilution", "Reciprocal of Dilution", "Factor", "ELISA Ab units")
kable(example)
```


## K.Miura method

K.Miura et.al. [@Miura2008] previously assigned Antibody Units to the STD (as the reciprocal dilution giving an OD~450nm~=1*). After this step, the STD was applied to each ELISA plate.

  (@) For example, for the **Anti-Pvs25 mokey sera**, after generating aliquotes of 1:100 from the initial pool, four-fold serial dilution was perfomed. Here, an OD~450nm~=1 had a Rec.Dilution of 20,000. Then STD Ab units was equal to 20,000. 

  (@) After the assignation of Ab units to the STD, a two-fold serial dilution starting with a dilution of 20 Ab units, equivalent to 1:1000 dilution, was applied on each ELISA plate.

  (@) As an example, in the paper is said that the 5th dilution of the STD sera is suppoused to have 1.25 Ab units. So, if $1.25=\frac{STD}{16000}$ then the STD Ab units are equal to 20,000.

*The reason behind an OD~450nm~=1 may be in the variability of the STD curve Upper and Lower limits. This behaviour also is observed in the comparoson of the STD OD values across ELISA plates.

## Corran method

Cook et.al.[@Cook2011] cite Corran et.al. [@Corran2008] which have a full description of the methods.

Corran et.al.[@Corran2008] make the assumption of an undiluted (initial) concentration of the sera pool previous to a 3pLL model fitting. By this way, the OD is directly proportional to the independent variable. [suppl.file 1](https://static-content.springer.com/esm/art%3A10.1186%2F1475-2875-7-195/MediaObjects/12936_2008_670_MOESM1_ESM.pdf)

At Methods, they write the following:

> "A titration curve was fitted to the ODs obtained for the standard plasma dilutions by least squares minimisation using a three variable sigmoid model and the solver add-in in Excel (Microsoft), assuming an arbitrary value of **1000 Units/ml** of antibody against each antigen in the **standard pool**. OD values for the spot extracts were converted to units/ml using this fitted curve."

***

# APPENDIX B: the 4pLL model

The **four-parameter log-logistic function** have the following presentations [@weimer2012impact]: $$f(x)=f(x;b,c,d,e)=c+\frac{d-c}{1+\exp[b(log(x)-log(e))]}=c+\frac{d-c}{1+\left(\frac{x}{e}\right)^b}=c+\frac{d-c}{1+{10}^{b(log_{10}(x)-log_{10}(e))}}$$

By isolating $x$, equivalent expressions are commonly reported as inverse functions of the 4pLL equation, assuming `b=-1` and then `c=0`:
$$x=e\left[{\left(\frac{d-f(x)}{f(x)-c}\right)}^{1/b}\right] \qquad \Rightarrow \qquad x=\frac{e}{\frac{d-c}{f(x)-c}-1} \qquad  \Rightarrow \qquad x=\frac{e}{\frac{d}{f(x)}-1}$$

If no log-transformation is required, then the following expression are often employed, depending on the scale of `x`: $$f(x)=c+\frac{d-c}{1+\exp[b(x-e)]} \qquad or \qquad f(x)=c+\frac{d-c}{1+{10}^{b(x-e)}}$$ For example, this last expression, with `x` in decimal-log scale, is present in the page 7 of the *drLumi* [@drLumi] package vignette.

Lastly, when using the nls function to fit the data, [Self-Starter](https://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Nonlinear-Regression.pdf) functions are required. This functions, in contrast with the last example, requires `x` in natural-log scale. E.g. the logistic model have the following parameters $$f(x)=\frac{\phi_1}{1+\exp\left[ \frac{\phi_2-x}{\phi_3}\right]} \qquad \Rightarrow \qquad f(x)=0+\frac{\phi_1-0}{1+\exp\left[ \frac{-1}{\phi_3} (x-\phi_2) \right]} $$

## Publications

  1. **Miura et.al, 2008** [@Miura2008] reported a "four-parameter hyperbolic curve" in this form: $$x=e\left[{\left(\frac{d-f(x)}{f(x)-c}\right)}^{1/b}\right]$$ where $x={Ab}_{units}$ and $f(x)={OD}_{450nm}$ without parameter letters definition. Those parameter letters have been changed in order to keep concordance with the extended description gave [above](#method).

  1. **Cook et.al., 2011 in Bioko Island, Equatorial Guinea** [@Cook2011] cite Corran et.al. [@Corran2008] which have a full description of the methods [suppl.file 1](https://static-content.springer.com/esm/art%3A10.1186%2F1475-2875-7-195/MediaObjects/12936_2008_670_MOESM1_ESM.pdf). Briefly, they report a ligand-binding equation, with the following inverse function: $$A=\frac{A_{max}*B}{B+k} \qquad \Rightarrow \qquad B=\frac{C}{D}=\frac{k}{\frac{A_{max}}{A}-1}$$ That equation takes a recognizable form after a rearrangement, showing a 4pLL assuming `c=0` and `b=-1`, the same as with the inverse form: $$A=0+\frac{A_{max}-0}{1+\left(\frac{B}{k}\right)^{-1}} \equiv f(x)=0+\frac{d-0}{1+\left(\frac{x}{e}\right)^b}  \qquad and \qquad B=\frac{k}{\frac{A_{max}-0}{A-0}-1} \equiv x=\frac{e}{\frac{d-0}{f(x)-0}-1} $$
    
  2. **Cunha & Sepulveda et.al., 2014 in Jacareacanga, Brazil** [@Cunha2014] report a formula without any details, as in the cited publication [@Bousema2010]: $$ {titer}= \frac{dilution}{ \frac{maximum.OD}{OD.test.serum - minimum.OD} -1} \qquad \neq \qquad x=\frac{e}{\frac{d-c}{f(x)-c}-1} $$ Eventually, they tried to write the inverse of the 4pLL model but fail in the description of the variables.
  

**CONCLUSION:**

Here I have shortly compare the equivalent expressions applied by several reference publications [@Miura2008] [@Cook2011] [@Cunha2014] [@Corran2008], all equal to the **four-parameter log-logistic function** applied in this report.

***

# APPENDIX C: residual NA

```{r}
#naMAT
#names(which(table(wb11_Ab$ID)!=2))[1]
naMATres <- NULL
for (i in 1:length(names(which(table(wb11_Ab$ID)!=2)))) {
  naMATres <- rbind(naMATres,naMAT[naMAT$ID==names(which(table(wb11_Ab$ID)!=2))[i],]) # -96
}
naMATres
#dim(naMATres)
#length(levels(factor(naMATres$ID)))
```

## re-run

- **03 samples** have OD above the upper limit:

```{r}
rbind(naMATres[naMATres$ID=="ZG163-2",],
      naMATres[naMATres$ID=="NN038-6",],
      naMATres[naMATres$ID=="LL031-6",])
```

### re-run covariates

In order to evaluate the posibility of a re-run, relevant sample covariates as age should be considered:

```{r}
rbind(finalDBcov[finalDBcov$id=="ZG163-2",-c(7,8,11)],
      finalDBcov[finalDBcov$id=="NN038-6",-c(7,8,11)],
      finalDBcov[finalDBcov$id=="LL031-6",-c(7,8,11)])
```

# APPENDIX D: Upper/Lower limits

```{r}
rownames(paramALL) <- d
paramALL
```

# APPENDIX E: Replicates

## PRE selection

### pre triplicates
```{r}
triplMAT
```

### pre duplicates
```{r}
dupliMAT
```


## POST selection

### pos triplicates
```{r}
triplNEW
```

### pos duplicates
```{r}
dupliNEW
```


# APPENDIX F: Updates

*09dic2016:*

With the Antibody Units:

- Conversion of the Estimated Reciprocal Dilutions to *Arbitrary Ab units*.
    + *SOL:* Followed NAMRU-6 SOP for ELISA data analysis.
    + A complementary description is in the [Appendix A](#appendix-a-from-od-to-ab-units)

- Complete understanding of Cook et.al. [@Cook2011] and Cunha et.al. [@Cunha2014] or Cook et.al. [@Cook2011] methodology.
    + *SOL:* A complementary description is in the [Appendix B](#appendix-b-the-4pll-model).

- Blank wells were added as STD with Ab units equal to zero. Then, a 4pLL was the best model to fit STD data.
    + *NOTE:* This allows to avoid background subtraction.
    
- Positive Control added to graph and Dose estimation
    + *NOTE:* ctrl+ out of STD range should be discussed.
    
*16dic2016:*

- A comparison between both states of Background Subtraction [ON] vs [OFF].
    + *SOL:* An [OFF] state after a Box-Cox transformation results in lower amount of NaN's among the first 5 plates.

- Addition of residual variance and QQ-plots before and after Box-Cox transformation.

- Histogram, Density plots and QQ-plots for OD vs Ab.unit comparison.
    + *NOTE:* No dramatic changes after Background Subtraction.

*23dic2016:*

- 2 main outputs:
    + 01 SIMPLE I/O report for all ELISA templates (w/o Background subtraction)
    + 01 EXTENDED report per ELISA template available if required

- Flowchart of methodology

- 04 Templates with lower samples than expected:
    + N12 (n=39), N18 (n=37), N23 (n=10), N41 (n=18).

- 01 Template with more samples than expected:
    + N15 (n=41)
        - **ZG181-1** sampled against **Pviv** plate only
        - **ZG182-1** sampled against **Pfal** plate only.
    + Both generated NaNs at the end. Needs to be solved.

- Some XLS file modifications:
    + from N20 to N37 and from N39 to N41, 03 extra rows after "Plate 1 QTSR Pf" were deleted.
    + N39 typo at cell M36 corrected.
    + N18 typo at cell J30 corrected.
    + from N23 to N26, STD cell format changed from 'character' to 'number'.
    + N41 blank and ctr+/- were moved to the right position.
    
*30dic2016:*

- Unbiased selection of Ab.units estimate among replicated samples:
    + *NOTE:* This assumes that no dilution to the original evaluated sample has been performed.
  
- About **73 NaN** Ab.units, since only one is above its model Upper Limit, I suggest to avoid all this measurement.
    + Requires discussion!
    
*06ene2017:*

- Improvement of the unbiased selection criteria:
    + For triplicates, the closest value to the mean among replicates was selected
    + Reads with mean.OD with a %CV higher than 20% were rejected
    + For duplicates, the replicate with lower %CV was preferred.
    + If a selection was not acomplished, the last measurement was preferred.
    + Criteria was implemented in a function called `Unbiased()`

- Ab.units data frame filtering:
    + Retrieve of all replicated reads: triplicates, duplicates and NaN.
    + Addition of selected reads among triplicates and duplicates.
    + Visualization of mean.OD-Ab.unit non-linearity and whole %CV before and after the procedure

- Brefly: 
    + 964 samples are **in** with 02 reads each, 01 per specie
    + 49 samples are **in** with only 01 read each
    + 03 samples are **out** because of NaN on both reads

- From the resting 55 reads with unestimated Ab.unit:
    + Only 03 reads have a mean.OD **above** their respective model upper limit
        + a re-run may be discussed.
    + All the rest, have a mean.OD **below** the lower limit
        + These reads aproaches to zero.

- Done with 02 main outputs:
    + a SIMPLE **.html** report to share with collaborators.
    + a FINAL **.csv** with the obtained Ab.units with the *Zungarococha_database1_3Oct2016.csv* file format
        + Better suited for **covariates** addition.

- For a comparison between resting NaN's mean.OD and Upper/Lower limits per ELISA plate:
    + Complete list of NaN's available in [Appendix C](#appendix-c-residual-na)
    + Complete list of Upper/Lower limits per ELISA plate in [Appendix D](#appendix-d-upperlower-limits)

- A complete list of previous updates available in [Appendix F](#appendix-f-updates)

*13ene2017:*

- **To discuss:** Criteria for NaN:
    + If approaching zero, then change NaN to 0? (n=46+6)
    + If aproaching infinity, then dilute and re-run? (n=3)
        + Review Corran, et al. criteria.
    
- The original covariate DB is required to fill then into the final DB
    + `Zungarococha_database1_30oct2016` have only **960 samples**
    + There is no data for **09 samples**, and
    + There are **02 samples** that are not present in the 28 evaluated Templates.

*20ene2017:*

- After Katty indication, DB typos were corrected in the current [script](#db-typo-corrections):
    + 2 samples were added: ZG009-3 and NN068-2.

- Complete list of replicates before and after the unbiased selection available at [APPENDIX E](#appendix-e-replicates)

- Covariate DB was added to the [analysis](#new-db-comparison):
    + Updated covariate DB was merged with the Ab.units per sample:
        + CSV file is available to re-analyse seropositivity in STATA.
    
    + Contrast of covariates and Ab.units between patients with doble ID?
        + Based on the current data, there is not enough support to delete those samples.

    + A comparison between covariate DB PRE and POST update is also available
        + Frequencies and age distribution per community. 

- **To discuss:** Criteria for NaN:
    + If approaching zero, then change NaN to 0? (n=46+6)
    + If aproaching infinity, then dilute and re-run? (n=3)

    
**PROBLEMS TO BE SOLVED:**

- *MUST:* Improve code
    + Replace SDCV by an extended MEAN data.frame.
    + This will solve N15 NaN problem



# Computer environment

```{r}
sessionInfo()
```


# References