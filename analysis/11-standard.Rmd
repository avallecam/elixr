---
title: "from Template to Standardized ELISA"
author: "Andree Valle Campos"
date: '`r Sys.Date()`'
output: 
  html_document:
#  pdf_document:
#  html_notebook:
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
    code_folding: "hide"
#    number_sections: TRUE
#    df_print: kable
#    fig_caption: true
#  documentclass: report
bibliography: SeroMarker.bib
csl: american-medical-association.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, # CHANGE TO FALSE IN PDF_DOCUMENTS
                      warning = TRUE)
knitr::opts_knit$set(root.dir = gsub("/analysis","",getwd()))
#getwd()
options(width = 140) # CHANGE TO DEFAULT in PDF
# FOR PDF CHANGE: html_document, toc_float, number_sections, echo, width
# FOR PDF add new page USE: four \newpage BEFORE #SETUP #APPENDIX #COMPUTER.env #REFERENCES
```

# Setup 

This report is written in **R** [@R] within a `Rmarkdown` [@rmarkdown] notebook using the `RStudio` [@rstudio] IDE software, and employing the `knitr` [@knitr] and `Hmisc` [@Hmisc] packages for the `html` setup.

This dynamic document integrates **text**, **code** and **results**[@CienciaReproducible2016].

```{r setup0, results='hide', message=FALSE, eval=TRUE}
require(Hmisc)
#require(plotly)
#options(#grType='plotly',  # for certain graphics functions
#        width = 110) # to expand the limits of CONSOLE output
mu <- markupSpecs$html   # markupSpecs is in Hmisc

# The following hidden command (<code>r mu$widescreen()</code>), causes the html notebook to use an entire wide screen.
#mu$widescreen()
```
`r mu$widescreen()`
 


# Summary

**OBJECTIVE:**

* Principal: 
    - Standardize the OD~450nm~ values across ELISA plates by the estimation of the Antibody Units of each unknown sample.

* Secondary:
    - Implement a [reproducible workflow](https://www.ncbi.nlm.nih.gov/pubmed/26776185) for standardization of ELISA plates from a template matrix.

**SOLVED PROBLEMS:**

*11nov2016:*

With the Reciprocals of Dilution:

- A 4pLL regression, without background subtraction, generated NaN's for OD values below the estimated Lower Limit of the model.
    + *SOL:* A 3pLL regression, after a background subtraction, solved this issue.

- The model reported by Miura et.al. [@Miura2008] seems to be different compared to the 4pLL model.
    + *SOL:* A short description of this equations is in the [Appendix B](#appendix-b-the-4pll-model).
    
- One .csv file as output: 
    1. standardized values

*24nov2016:*

- A pseudo-$R^2$ index to measure the goodness-of-fit of *logistic regression models* as suggested [here](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm) was missing:
    + *SOL:* usage of nagelkerke {rcompanion} function after a model fitted by nls {stats} function.

- One more .csv file output added: 
    2. mean blank vs pseudo-$R^2$ values.

*09dic2016:*

With the Antibody Units:

- Conversion of the Estimated Reciprocal Dilutions to *Arbitrary Ab units*.
    + *SOL:* Followed NAMRU-6 SOP for ELISA data analysis.
    + A complementary description is in the [Appendix A](#appendix-a-from-od-to-ab-units)

- Complete understanding of Cook et.al. [@Cook2011] and Cunha et.al. [@Cunha2014] or Cook et.al. [@Cook2011] methodology.
    + *SOL:* A complementary description is in the [Appendix B](#appendix-b-the-4pll-model).

- OLS $R^2$ for Linear regressions was implemented. Cox-Snell pseudo-$R^2$ was almost equal in all cases.
    + *SOL:* A short description of this is in the [Appendix C](#appendix-c-r-square).
    + *NOTE:* Theory behind its vulnerability was discussed.

- Blank wells were added as STD with Ab units equal to zero. Then, a 4pLL was the best model to fit STD data.
    + *NOTE:* This allows to avoid background subtraction.
    
- Positive Control added to graph and Dose estimation
    + *NOTE:* C+ out of STD range should be discussed.

- One more .csv file output added: 
    3. standard curve OD values.
    
*16dic2016:*

- Addition of conditional outputs depending on the state of Background Subtraction [ON] or [OFF].

- A comparison between both states of Background Subtraction [ON] vs [OFF].
    + *SOL:* An [OFF] state after a Box-Cox transformation results in lower amount of NaN's among the first 5 plates.

- Addition of residual variance and QQ-plots before and after Box-Cox transformation.

- Histogram, Density plots and QQ-plots for OD vs Ab.unit comparison.
    + *NOTE:* No dramatic changes after Background Subtraction.
    + *Q:* Ab.units previous to mean(Ab.units)? or mean(OD) previous to Ab.units?

- write.csv() functions locked.

**PROBLEMS TO BE SOLVED:**

- Flowchart of methodology (R diagrams)

- A tidy .html report to share with Nuno Sepulveda.

- A threshold criteria of C+/- is required to suggest any re-run.

- 2 main outputs:
    + 28 FULL report per each ELISA templates
    + 01 SIMPLE I/O report for all ELISA templates (w/o Background subtraction)

- Consider an additional format to the 1st output.
    + One row of both PvMSP1 and PfMSP1 standardized values per sample.
    + AIM: Conserve *Zungarococha_database1_3Oct2016.csv* file format


**NEXT STEPS:**

- Automatically generate all the files copies to make the analysis of all the ELISA plates. 
    + Use a bash/perl script on a Unix terminal.

- Bind all the output .csv files.
    - **Compare** the distribution of all plates OD vs standardized measurements.
    - **Evaluate** *Quality Control plot*: mean.OD blank values vs $R^2$ of STD curves.
    - **Compare** STD OD variability across plates
    - **Visualize** the %CV of all the data.

- Replicate Nuno's analysis with Christian's script in STATA.


# Dependencies

Here are the required R packages for this analysis:
```{r, results='hide', message=FALSE}
##essential
library(XLConnect)    # load EXCEL workbooks
library(drc)          # Dose-Response modeling
##accesory
library("rcompanion") # pseudo R-2
library("DiagrammeR") # method Flowchart
library("knitr")      # To display nice tables
library("tidyverse")
#ggplot2, tibble, tidyr, readr, purr, dplyyr
##extra
library("Rmisc")      # multiplot ggplots
library("haven")      # import STATA df
library("readxl")     # import EXCEL df
library("forcats")    # factor vectors
library("viridis")    # color visualization
```


# Introduction

This is an R implementation of a pipeline for the standardization of ELISA plates directly from a template matrix.

By changing only all the **"N11"** words of this file with its respective **"N##"**, all the other **Template_ELISA_"N##".xlsx** files will be ready for a *knit* in Rstudio and later visualization of the analysis.



# Method

El reporte permite ver, paso a paso, el procedimiento aplicado. Un resumen lo explicara a continuacion:

Primero, se presentara la **distribucion** de la curva estandar presente en cada placa de ELISA, la cual permitira justificar la ejecucion de una adecuada estimacion de parametros.

Luego, se ejecutara la **regresion no lineal** para la estimacion de los cuatro parametros del modelo **log-logistico simetrico** (limite superior, pendiente, punto de inflexion y limite inferior), empleando el paquete **drc** [@Ritz2015]. Las siglas hacen referencia a *dose-response curves*.

A partir de este modelo, se estimarán las **Unidades Arbitrarias de Anticuerpos** a partir de los OD~450nm~ de muestras desconocidas.

Finalmente, se obtendrá como **output** tres files en .csv: 

1. tabla con valores estimados por muestra por especie,
2. tabla con valores de Control de Calidad para cada placa.
3. tabla con valores de la Curva Estandar por placa

```{r, fig.align='center', fig.width=9}
#install.packages("DiagrammeR")
#library(DiagrammeR)

DiagrammeR("
  graph LR
    A[XLS data] -.-> |XLConnect| B{R data}
    B --> C1[STD]
    B --> E[ctr +/-]
    B --> D1[UNK]
    
    C1 -.-> |drc| C2[4pLL model]
    C2 --> C3[Box-Cox]
    C3 --> F{UNK Ab.units}

    D1 --> D2[mean.OD]
    D2 --> D3[OD %CV]
    D3 --> F
    
    F --> G1[Histogram]
    F --> G2[Density]
    F --> G3[QQPlot]

    style A fill:#ffffff, stroke:#000000, stroke-width:2px
    style B fill:#ffffff, stroke:#000000, stroke-width:2px
    style C1 fill:#ffffff, stroke:#000000, stroke-width:2px
    style C2 fill:#ffffff, stroke:#000000, stroke-width:2px
    style C3 fill:#ffffff, stroke:#000000, stroke-width:2px
    style D1 fill:#ffffff, stroke:#000000, stroke-width:2px
    style D2 fill:#ffffff, stroke:#000000, stroke-width:2px
    style D3 fill:#ffffff, stroke:#000000, stroke-width:2px
    style E fill:#ffffff, stroke:#000000, stroke-width:2px
    style F fill:#ffffff, stroke:#000000, stroke-width:2px
    style G1 fill:#ffffff, stroke:#000000, stroke-width:2px
    style G2 fill:#ffffff, stroke:#000000, stroke-width:2px
    style G3 fill:#ffffff, stroke:#000000, stroke-width:2px
")
```

# ELISA data: **Plate N11**

## Input: **.xlsx workbooks**

The conserved pattern of data across files allowed me to load the required information by giving the coordinates of the tables to the R package.

Note that I have changed the name of the files to a more easy-to-read pattern:
```{r input}
#wb11 <- loadWorkbook(path.expand("~/Documents/Valle_GnB/000_R_script_MIRRORS/R_test/SeroMarkerMir/data-raw/Template_ELISA_N11.xlsx"))
#wb11 <- loadWorkbook(paste0(getwd(),"/data-raw/Template_ELISA_N11.xlsx"))
wb11 <- loadWorkbook("data-raw/Template_ELISA_N11.xlsx")

wb11_conditions <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 9, endRow =18,
                         startCol = 3, endCol = 4)

wb11_Pf_main <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 22, endRow = 30,
                         startCol = 3, endCol = 14)
wb11_Pf <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 33, endRow = 41,
                         startCol = 3, endCol = 14)
wb11_Pv_main <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 79, endRow = 87,
                         startCol = 3, endCol = 14)
wb11_Pv <- readWorksheet(wb11, 
                         sheet = 1, 
                         startRow = 90, endRow = 98,
                         startCol = 3, endCol = 14)

```

**Conditions** of the experimient
```{r}
colnames(wb11_conditions)<-c("","CONDITIONS: Template ELISA N11")
kable(wb11_conditions)
```


**Sample ID** matrix of the plates
```{r}
wb11_Pf_main
```

A logic test to evaluate if Pfal and Pviv ELISA plates have the same **sample ID distribution**.
```{r}
wb11_Pf_main == wb11_Pv_main # Are ID the same between speacies plates?
```

A visualization of both data matrix with **OD values**
```{r}
#wb11_Pf_main
wb11_Pf
#wb11_Pv_main
wb11_Pv
```

## STD & CTRL distribution

In order to work with the data, both *Sample ID* and *OD values* matrix were merged into one single table or *data frame*.

"Click" the `Code` tab of the right to see the procedure:
```{r}
#str(wb11_Pf)
#class(wb11_Pf)

Pf.OD <- matrix(wb11_Pf[1,])
Pf.ID <- matrix(wb11_Pf_main[1,])
Pv.OD <- matrix(wb11_Pv[1,])
Pv.ID <- matrix(wb11_Pv_main[1,])

for (i in 2:nrow(wb11_Pf)) {
  Pf.OD <- rbind(Pf.OD,matrix(wb11_Pf[i,]))
  Pf.ID <- rbind(Pf.ID,matrix(wb11_Pf_main[i,]))
  Pv.OD <- rbind(Pv.OD,matrix(wb11_Pv[i,]))
  Pv.ID <- rbind(Pv.ID,matrix(wb11_Pv_main[i,]))
}
#Pf.OD; Pf.ID; Pv.OD; Pv.ID

#wb11_Pf_main[1,]

#
t<-seq(1,12,1)
x <- c()
x0 <- 50
f <- function(x){x*2}
x[1]<-x0
for (i in 1:(length(t)-1)) {
  x[i+1] = f(x[i])
}
#x
#

wb11_ALL <- data.frame("N11",
                      rbind(Pf.ID,Pv.ID),
                      rbind(matrix(rep("std",12)),
                            matrix(rep("unk",80)),"std",
                            matrix(rep("ctr",2)),"std"), 
                      rep(x[length(x)]/x), ##FACTOR for AB.UNITS
                      rbind(Pf.OD,Pv.OD), 
                      gl(2,96, labels = c("Pfal","Pviv")))
colnames(wb11_ALL) <- c("Plate", "ID", "Type", "Ab.unit", "OD", "Specie")
wb11_ALL$ID <- unlist(wb11_ALL$ID)
wb11_ALL$OD <- unlist(wb11_ALL$OD)
wb11_ALL$ID <- factor(wb11_ALL$ID) # unk goes from 
#levels(wb11_ALL$ID); levels(wb11_ALL$ID)[16]; levels(wb11_ALL$ID)[55]
#
wb11_ALL <- wb11_ALL[complete.cases(wb11_ALL[,2]),]
#
#wb11_ALL[c(13:96,109:192),"Ab.unit"] <- NA
wb11_ALL[wb11_ALL$Type!="std","Ab.unit"] <- NA
wb11_ALL[wb11_ALL$ID=="Blank","Ab.unit"] <- 0
#wb11_ALL
#
wb11_ALL$OD <- as.numeric(wb11_ALL$OD)
#
```

Here is a list of the **column names** of the created data frame:
```{r}
str(wb11_ALL)
```

A full visualization of data frame at this step is in the [Appendix D](#appendix-d-data-frame-per-steps):
```{r}
wb11_RAW <- wb11_ALL
#head(wb11_RAW)
```

A visualization of the **control** samples (blank, positive and negative controls) per specie:
```{r}
subset(wb11_ALL, Type=="ctr")
#tail(wb11_ALL[rev(order(wb11_ALL$ID)),], 8)

blank.Pfal <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="Blank" & wb11_ALL$Specie=="Pfal",5]))
blank.Pviv <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="Blank" & wb11_ALL$Specie=="Pviv",5]))

ctrPlu.Pfal <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C+" & wb11_ALL$Specie=="Pfal",5]))
ctrPlu.Pviv <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C+" & wb11_ALL$Specie=="Pviv",5]))

ctrNeg.Pfal <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C-" & wb11_ALL$Specie=="Pfal",5]))
ctrNeg.Pviv <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C-" & wb11_ALL$Specie=="Pviv",5]))

```

A visualization of the **reference serum** or **standard** of each ELISA plate:
```{r}
subset(wb11_ALL, Type=="std")
```

### Visualization

Data distribution of the **reference serum** or **standard** on each ELISA plate in *linear* and *log-scale*:
```{r, fig.width=8, fig.height=4, fig.align='center'}
#

par(mfrow=c(1,2))

plot(OD ~ Ab.unit, 
     data=subset(wb11_ALL, Type=="std" & Specie=="Pfal"), 
     ylim = c(0,1.5), col="red",
     xlab = "Ab unit", ylab = "OD 450nm",
     main="Linear scale")
points(OD ~ Ab.unit, 
       data=subset(wb11_ALL, Type=="std" & Specie=="Pviv"), 
       pch=2, col="blue")
abline(h=c(ctrNeg.Pfal, blank.Pfal, ctrNeg.Pviv, blank.Pviv), lty=c(3, 2, 3, 2), col=c("red","red","blue","blue"))
abline(h=c(ctrPlu.Pfal, ctrPlu.Pviv), lty=c(3, 3), col=c("red", "blue"))
legend("bottomright", c("P.vivax","P.falcip", "C+/-", "blank"), 
       col = c( "blue", "red", "black", "black"), 
       pch = c(2, 1, NA, NA), #lwd= c(1,1),
       lty = c(NA, NA, 3, 2),
       cex = 0.8,
       inset = .01#, merge = TRUE
)

plot(OD ~ log(Ab.unit), 
     data=subset(wb11_ALL, Type=="std" & Specie=="Pfal"), 
     ylim = c(0,1.5), col="red",
     xlab = "log(Ab unit)", ylab = "OD 450nm",
     main="Log scale")
points(OD ~ log(Ab.unit), 
       data=subset(wb11_ALL, Type=="std" & Specie=="Pviv"), 
       pch=2, col="blue")
abline(h=c(ctrNeg.Pfal, blank.Pfal, ctrNeg.Pviv, blank.Pviv), lty=c(3, 2, 3, 2), col=c("red","red","blue","blue"))
abline(h=c(ctrPlu.Pfal, ctrPlu.Pviv), lty=c(3, 3), col=c("red", "blue"))
legend("bottomright", c("P.vivax","P.falcip", "C+/-", "blank"), 
       col = c( "blue", "red", "black", "black"), 
       pch = c(2, 1, NA, NA), #lwd= c(1,1),
       lty = c(NA, NA, 3, 2),
       cex = 0.8,
       inset = .01#, merge = TRUE
)

par(mfrow=c(1,1))
```


## OD variability

Here is a matrix with the frequency of samples among the plates. **4 means** that there are **2 replicates** per Plasmodium specie plate.
```{r}
#head(wb11_ALL)
wb11_UNK <- subset(wb11_ALL, Type=="unk")
wb11_UNK$ID <- factor(wb11_UNK$ID) # unk goes from 

n <- length(levels(wb11_UNK$ID))

table(wb11_UNK$ID)
```

Both plates have `r n` of 40 expected samples

**MISSING:** A conditional function to extract samples with no duplicates or unique in a plate
```{r, eval=FALSE}
table(wb11_UNK$ID)<4
```

### Arithmetic mean of duplicates

A table of all unknown samples is make with empty mean.OD and Ab.unit to be fill:
```{r}
#GET mean of all duplicates
wb11_MEAN <- data.frame("N11",
                        levels(wb11_UNK$ID),
                        rep("unk",length(levels(wb11_UNK$ID))), 
                        rep(NA,length(levels(wb11_UNK$ID))),#Ab.unit
                        rep(NA,length(levels(wb11_UNK$ID))),#mean.OD
                        gl(2,length(levels(wb11_UNK$ID)), 
                           labels = c("Pfal","Pviv")))
colnames(wb11_MEAN) <- c("Plate", "ID", "Type", "Ab.unit", "mean.OD", "Specie")
head(wb11_MEAN)
```

A full display of this table is in the [Appendix D](#appendix-d-data-frame-per-steps):

A table with the **raw** mean values filled is also available *only for comparison with .xlsx*

```{r}
#wb11_ALL$ID <- factor(wb11_ALL$ID) # unk goes from 1 + 16:55
#levels(wb11_ALL$ID)[1]; levels(wb11_ALL$ID)[16]; levels(wb11_ALL$ID)[55]
#levels(wb11_UNK$ID)[1]; levels(wb11_UNK$ID)[n]

#n <- length(levels(wb11_UNK$ID))

for (i in 1:n) {
  wb11_MEAN[i,5] <- mean(as.numeric(subset(wb11_UNK, 
                                           ID==levels(wb11_UNK$ID)[i] &
                                             Specie==levels(wb11_UNK$Specie)[1])$OD))
  wb11_MEAN[n+i,5] <- mean(as.numeric(subset(wb11_UNK, 
                                             ID==levels(wb11_UNK$ID)[i] &
                                               Specie==levels(wb11_UNK$Specie)[2])$OD))
}

head(wb11_MEAN)

wb11_MEAN_NO_SUB <- wb11_MEAN

#head(wb11_MEAN_NO_SUB)
```

### **[OFF]** Background Subtraction

* A 4pLL regression, without background subtraction, generated NaN's for OD values below the estimated Lower Limit of the model.
    + *SOL:* A 3pLL regression, after a background subtraction, solved this issue.

A full visualization of the subtracted data is display at this step is in the [Appendix D](#appendix-d-data-frame-per-steps):
```{r, eval=FALSE}

#IF off THEN FALSE
#IF ON THEN TRUE

#blank.Pfal
#blank.Pviv

wb11_ALL[wb11_ALL$Specie=="Pfal",5] <- wb11_ALL[wb11_ALL$Specie=="Pfal",5] - blank.Pfal
#head(wb11_ALL[wb11_ALL$Specie=="Pfal",])

wb11_ALL[wb11_ALL$Specie=="Pviv",5] <- wb11_ALL[wb11_ALL$Specie=="Pviv",5] - blank.Pviv
#head(wb11_ALL[wb11_ALL$Specie=="Pviv",])

wb11_RAW_SUB <- wb11_ALL
#head(wb11_SUB)

###############################################

#head(wb11_ALL)
wb11_UNK <- subset(wb11_ALL, Type=="unk")
wb11_UNK$ID <- factor(wb11_UNK$ID) # unk goes from 

n <- length(levels(wb11_UNK$ID))

############################################# after subtraction, apply the mean
for (i in 1:n) {
  wb11_MEAN[i,5] <- mean(subset(wb11_UNK, 
                                ID==levels(wb11_UNK$ID)[i] &
                                  Specie==levels(wb11_UNK$Specie)[1])$OD)
  wb11_MEAN[n+i,5] <- mean(subset(wb11_UNK, 
                                  ID==levels(wb11_UNK$ID)[i] &
                                    Specie==levels(wb11_UNK$Specie)[2])$OD)
}

head(wb11_MEAN)

wb11_SUB <- wb11_MEAN
#head(wb11_SUB)

#head(wb11_MEAN_NO_SUB) == head(wb11_SUB)
```


### SD and %CV of OD values
According to [reference](http://www.abcam.com/protocols/calculating-and-evaluating-elisa-data), the Coefficient of Variation (%CV) is defined as $$CV=\frac{\sigma}{\mu}*100$$.

Over the table above, we add two more columns, sd.OD and cv.OD, with empty spaces:
```{r}
wb11_SDCV <- cbind(wb11_MEAN, 
                   rep(NA,length(levels(wb11_UNK$ID))),#mean.OD
                   rep(NA,length(levels(wb11_UNK$ID))))#sd.OD)
colnames(wb11_SDCV) <- c("Plate", "ID", "Type", "Ab.unit", "mean.OD", "Specie", "sd.OD", "cv.OD")
head(wb11_SDCV)
```

A full display of this table is in the [Appendix D](#appendix-d-data-frame-per-steps):
```{r}
#GET sd of all duplicates

for (i in 1:n) {
  wb11_SDCV[i,7] <- sd(subset(wb11_UNK, 
                              ID==levels(wb11_UNK$ID)[i] &
                                Specie==levels(wb11_UNK$Specie)[1])$OD)
  wb11_SDCV[n+i,7] <- sd(subset(wb11_UNK, 
                                ID==levels(wb11_UNK$ID)[i] &
                                  Specie==levels(wb11_UNK$Specie)[2])$OD)
}

wb11_SDCV[,8] <- 100*(wb11_SDCV[,7] / wb11_SDCV[,5])

wb11_SDCV_PRE <- wb11_SDCV
head(wb11_SDCV_PRE)
#wb11_MEAN$mean.OD
```

The **maxima** %CV of duplicates is **`r max(wb11_SDCV_PRE$cv.OD, na.rm=TRUE)`%**

### Visualization of %CV
```{r, fig.width=5, fig.height=4, fig.align='center'}

if (sum(wb11_MEAN_NO_SUB$mean.OD, na.rm=TRUE) != sum(wb11_MEAN$mean.OD, na.rm=TRUE)) {
  plot(cv.OD ~ mean.OD,wb11_SDCV, 
     ylim=c(0,300), xlim=c(0,1.5),
     xlab="mean OD 450nm of duplicates",
     ylab="%CV of duplicates",
     main="Relationship between ELISA OD 450nm and \n Percentage Coefficient of Variation")
  abline(h=20, v=0.25, lty=2, col=2) 
} else {
   plot(cv.OD ~ mean.OD,wb11_SDCV, 
     ylim=c(0,100), xlim=c(0,1.5),
     xlab="mean OD 450nm of duplicates",
     ylab="%CV of duplicates",
     main="Relationship between ELISA OD 450nm and \n Percentage Coefficient of Variation")
  abline(h=20, v=0.25, lty=2, col=2)
}
```



### Negative controls

```{r}
ctrNeg.Pfal <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C-" & wb11_ALL$Specie=="Pfal",5]))
ctrNeg.Pviv <- mean(as.numeric(wb11_ALL[wb11_ALL$ID=="C-" & wb11_ALL$Specie=="Pviv",5]))
```

About negative control: 

- Among the *P.falciparum* unknown samples, `r sum(wb11_ALL[wb11_ALL$Specie=="Pfal" & wb11_ALL$Type=="unk",5] < ctrNeg.Pfal, na.rm = TRUE)` of `r length(wb11_ALL[wb11_ALL$Specie=="Pfal" & wb11_ALL$Type=="unk",5])` (`r (sum(wb11_ALL[wb11_ALL$Specie=="Pfal" & wb11_ALL$Type=="unk",5] < ctrNeg.Pfal, na.rm = TRUE)/length(wb11_ALL[wb11_ALL$Specie=="Pfal" & wb11_ALL$Type=="unk",5]))*100`%) have lower OD values than the Negative Control  (OD=`r ctrNeg.Pfal`).

- In the case of *P.vivax*, `r sum(wb11_ALL[wb11_ALL$Specie=="Pviv" & wb11_ALL$Type=="unk",5] < ctrNeg.Pviv, na.rm = TRUE)` of `r length(wb11_ALL[wb11_ALL$Specie=="Pviv" & wb11_ALL$Type=="unk",5])` (`r (sum(wb11_ALL[wb11_ALL$Specie=="Pviv" & wb11_ALL$Type=="unk",5] < ctrNeg.Pviv, na.rm = TRUE)/length(wb11_ALL[wb11_ALL$Specie=="Pviv" & wb11_ALL$Type=="unk",5]))*100`%) unknown samples have lower OD values than the Negative Control (OD=`r ctrNeg.Pviv`).


## Parameter estimation

La curva del modelo **log-logístico simétrico** describe la *respuesta* `f(x)` dependiente de cuatro parámetros $$ f(x)=c+\frac{d-c}{1+\exp[b(log(x)-log(e))]}\ $$ donde: 
  
- `c` es el **límite inferior** de la respuesta cuando la *dosis* `x` se aproxima al infinito, 
- `d` el **límite superior** cuando la *dosis* `x` se aproxima a cero,
- `b` es la **pendiente** alrededor del **punto de inflexión**, representado por 
- `e` de acuerdo al contexto de su evaluación.

### 4pLL model
```{r}

##------------ curve fitting -------------##
wb11.m1 <- drm(OD ~ Ab.unit, Specie, 
               data= subset(wb11_ALL, Type=="std"),
               fct = LL.4(names = c("b", 
                                    "c", 
                                    "d", "e")))
summary(wb11.m1)
```

### Visualization
```{r, fig.width=6, fig.height=4, fig.align='center'}
plot(wb11.m1, broken = TRUE, 
     xlab="Ab units", ylab="OD 450nm",
     ylim = c(0,1.5), 
     xlim = c(0,5e3),
     main="Standard Curve \n with Ab units and fitted Log-Logistic model", 
     col = c("red", "blue"),
     legendPos = c(2,1.5))
```

## Goodness-of-Fit

A short description of the theory for this section is in the [Appendix C](#appendix-c-r-square).

```{r}
wb11.model <- wb11.m1
```

```{r}
nn <- length(residuals(wb11.model))
R.sq_Pfal <- 1-sum((residuals(wb11.model)[1:(nn/2)])^2)/
  sum((wb11.model$data$OD[1:(nn/2)]-mean(wb11.model$data$OD[1:(nn/2)]))^2)
R.sq_Pviv <- 1-sum((residuals(wb11.model)[((nn/2)+1):nn])^2)/
  sum((wb11.model$data$OD[((nn/2)+1):nn]-mean(wb11.model$data$OD[((nn/2)+1):nn]))^2)
```

Under the **OLS R-squared** definition [@anderson1994model], 

- $R^2=$ **`r 1-sum((residuals(wb11.model)[1:(nn/2)])^2)/sum((wb11.model$data$OD[1:(nn/2)]-mean(wb11.model$data$OD[1:(nn/2)]))^2)`** for P. faciparum
- $R^2=$ **`r 1-sum((residuals(wb11.model)[((nn/2)+1):nn])^2)/sum((wb11.model$data$OD[((nn/2)+1):nn]-mean(wb11.model$data$OD[((nn/2)+1):nn]))^2)`** for P. vivax

Under the **pseudo R-square** definitions suggested [here](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm),

```{r}
wb11_ALL[wb11_ALL$ID=="Blank","Ab.unit"] <- 0.001
```

```{r, message=FALSE}
##**nls P.falciparum**
dataPf <- subset(wb11_ALL, Type=="std" & Specie=="Pfal")
dataPf$log.conc <- log(dataPf$Ab.unit)
#
dataPf.LL <- nls(OD ~ SSfpl(log.conc, A, B, xmid, scal), dataPf)
#summary(dataPf.LL)
#
nullfunct = function(x, m){m}
null.model = nls(OD ~ nullfunct(log.conc, m),
                 data = dataPf,
                 start = list(m   = 0.6))
#summary(null.model)

#library(rcompanion)  #### REQUIRES a null model ¡DONE!
#nagelkerke(dataPf.LL, null = null.model)$Pseudo.R.squared.for.model.vs.null
```

```{r, message=FALSE}
##**nls P.vivax**
dataPv <- subset(wb11_ALL, Type=="std" & Specie=="Pviv")
dataPv$log.conc <- log(dataPv$Ab.unit)
#
dataPv.LL <- nls(OD ~ SSfpl(log.conc, A, B, xmid, scal), dataPv)
#summary(dataPv.LL)
#
nullfunct = function(x, m){m}
null.model = nls(OD ~ nullfunct(log.conc, m),
                 data = dataPv,
                 start = list(m   = 0.6))
#summary(null.model)

#library(rcompanion)    #### REQUIRES a null model ¡DONE!
#nagelkerke(dataPv.LL, null = null.model)$Pseudo.R.squared.for.model.vs.null
```

```{r, fig.width=9, fig.height=6, fig.align='center', message=FALSE, eval=FALSE}
#### nls visualization
par(mfrow=c(3,3))

#########################

plotFit(dataPf.LL, interval = "confidence", 
        xlab="log(Ab units)", ylab="OD 450nm",
        ylim = c(0,1.5), 
        xlim = c(0,16),
        main="Standard Curve \n P.falciparum", 
        col = "red")

########################

r <- data.frame(residuals= residuals(dataPf.LL), predicted= predict(dataPf.LL))
plot(residuals ~ predicted, data= r, 
     main="Variance Hetereogeneity \n residuals vs fitted")
fit <- loess(residuals ~ predicted, data= r)
y <- seq(0,1.2,0.1)
lines(y, predict(fit, newdata= data.frame(predicted= y)), col="red", lwd=3)
abline(h=0)

qqnorm(residuals(dataPf.LL))
qqline(residuals(dataPf.LL))

########################
plotFit(dataPv.LL, interval = "confidence", 
        xlab="log(Ab units)", ylab="OD 450nm",
        ylim = c(0,1.5), 
        xlim = c(0,16),
        main="Standard Curve \n P.vivax", 
        col = "blue")

########################

r <- data.frame(residuals= residuals(dataPv.LL), predicted= predict(dataPv.LL))
plot(residuals ~ predicted, data= r, 
     main="Variance Hetereogeneity \n residuals vs fitted")
fit <- loess(residuals ~ predicted, data= r)
y <- seq(0,1.2,0.1)
lines(y, predict(fit, newdata= data.frame(predicted= y)), col="red", lwd=3)
abline(h=0)

qqnorm(residuals(dataPv.LL))
qqline(residuals(dataPv.LL))

#### nls Dose estimation

#**e.g.**

#Here the inverse estimate and a confidence interval are computed for the unknown predictor value that corresponds to an observed value of the response.

############################

#invest(dataPf.LL, y0 = 0.6, interval= "inversion")
log.estimate= format(invest(dataPf.LL, y0 = 0.6, interval= "Wald")$estimate, digits = 4)
estimate= format(exp(invest(dataPf.LL, y0 = 0.6, interval= "Wald")$estimate), digits = 6)
lower= format(exp(invest(dataPf.LL, y0 = 0.6, interval= "Wald")$lower), digits = 6)
upper= format(exp(invest(dataPf.LL, y0 = 0.6, interval= "Wald")$upper), digits = 6)
#
kable(cbind(P.fal= "y0 = 0.6", log.estimate, estimate,lower,upper))

############################

#invest(dataPv.LL, y0 = 0.6, interval= "inversion")
log.estimate= format(invest(dataPv.LL, y0 = 0.6, interval= "Wald")$estimate, digits = 4)
estimate= format(exp(invest(dataPv.LL, y0 = 0.6, interval= "Wald")$estimate), digits = 6)
lower= format(exp(invest(dataPv.LL, y0 = 0.6, interval= "Wald")$lower), digits = 6)
upper= format(exp(invest(dataPv.LL, y0 = 0.6, interval= "Wald")$upper), digits = 6)
#
kable(cbind(P.viv= "y0 = 0.6", log.estimate, estimate,lower,upper))
```

```{r}
wb11_ALL[wb11_ALL$ID=="Blank","Ab.unit"] <- 0
```

- Cox and Snell (ML) pseudo-$R^2=$ **`r nagelkerke(dataPf.LL, null = null.model)$Pseudo.R.squared.for.model.vs.null[2]`** for P. faciparum
- Cox and Snell (ML) pseudo-$R^2=$ **`r nagelkerke(dataPv.LL, null = null.model)$Pseudo.R.squared.for.model.vs.null[2]`** for P. vivax

Lastly, if a **Generalized R square** is defined under a context of a Model comparison [@anderson1994model]:

- **AIC:**
```{r}
##### lack of fit test IS NOT POSSIBLE since you only have ONE POINT DILUTION
#modelFit(wb11.m1) #DONT WORK -> NOW works but 2 DF vs 20 DF
#anova(wb11.m1) #DONT WORK

#- **ANOVA** test discrepancies between mean regression values of two different [models](http://rstats4ag.org/dose-response/#More_Dose-Response_Curves)

mselect(wb11.m1, list(LL.3(),LL.5()), linreg = TRUE)
```

- **LRT:**
```{r}
noEffect(wb11.m1)
```



## Assumptions evaluation

- **BoxCox transformation** must be performed in case **heterogenious variance of residuals**. [more](http://onlinestatbook.com/2/transformations/box-cox.html)
- After UNK Ab.units estimation, more NaN's were solved using this transformed model without Background subtraction than with it.

```{r, fig.width=9, fig.height=6, fig.align='center'}
##--------------residuals----------------##

par(mfrow=c(2,3))

plot(residuals(wb11.model) ~ fitted(wb11.model), 
     main="Variance Hetereogeneity \n residuals vs fitted")
abline(h=0)
#qqplot(residuals(wb11.model))
#qqline(residuals(wb11.model))

##------Box-Cox transformation-------##
wb11.model.BX <- boxcox(wb11.model, 
                     main=expression("Optimal " ~ lambda ~ " with confidence intervals"))
summary(wb11.model.BX)

plot(residuals(wb11.model.BX) ~ fitted(wb11.model.BX), 
     main="Variance Homogeneity \n residuals vs fitted")
abline(h=0)

qqnorm(residuals(wb11.model))
qqline(residuals(wb11.model))

plot(wb11.model, #broken = TRUE, 
     xlab="Ab units", ylab="OD 450nm",
     ylim = c(0,1.5), 
     xlim = c(0,5e3),
     main="Standard Curve \n in red: BoxCox transformed model",
     legendPos = c(2,1.5))
plot(wb11.model.BX, col = "red", add = TRUE, legend = FALSE,
     ylim = c(0,1.5), 
     xlim = c(0,5e5))

qqnorm(residuals(wb11.model.BX))
qqline(residuals(wb11.model.BX))

```

**R-squared of Box-Cox transformed models**

```{r}
nn.BX <- length(residuals(wb11.model.BX))
R.sq_Pfal_BX <- 1-sum((residuals(wb11.model.BX)[1:(nn.BX/2)])^2)/
  sum((wb11.model.BX$data$OD[1:(nn.BX/2)]-mean(wb11.model.BX$data$OD[1:(nn.BX/2)]))^2)
R.sq_Pviv_BX <- 1-sum((residuals(wb11.model.BX)[((nn.BX/2)+1):nn.BX])^2)/
  sum((wb11.model.BX$data$OD[((nn.BX/2)+1):nn.BX]-mean(wb11.model.BX$data$OD[((nn.BX/2)+1):nn.BX]))^2)
```

Under the OLS R-squared definition, Box-Cox transformed models have:

- $R^2=$ **`r 1-sum((residuals(wb11.model.BX)[1:(nn.BX/2)])^2)/sum((wb11.model.BX$data$OD[1:(nn.BX/2)]-mean(wb11.model.BX$data$OD[1:(nn.BX/2)]))^2)`** for P. faciparum
- $R^2=$ **`r 1-sum((residuals(wb11.model.BX)[((nn.BX/2)+1):nn.BX])^2)/sum((wb11.model.BX$data$OD[((nn.BX/2)+1):nn.BX]-mean(wb11.model.BX$data$OD[((nn.BX/2)+1):nn.BX]))^2)`** for P. vivax

For comparison, untrasformed model have:

- $R^2=$ **`r 1-sum((residuals(wb11.model)[1:(nn/2)])^2)/sum((wb11.model$data$OD[1:(nn/2)]-mean(wb11.model$data$OD[1:(nn/2)]))^2)`** for P. faciparum
- $R^2=$ **`r 1-sum((residuals(wb11.model)[((nn/2)+1):nn])^2)/sum((wb11.model$data$OD[((nn/2)+1):nn]-mean(wb11.model$data$OD[((nn/2)+1):nn]))^2)`** for P. vivax


```{r, eval=FALSE}
### Model comparison
mselect(wb11.model.BX,list(LL.3(),LL.5()))
#anova(wb11.model, wb11.model.BX) #the test is NOT SIGNIFICANT, allowing us to assume that both curves have the same SLOPE, LOWER limit, and UPPER limit.
AIC(wb11.model, wb11.model.BX) # CRITERIA: SMALLER values are better!
#lrtest(wb11.model.BX, wb11.model) #GENERAL vs SIMPLER
```


## Model resume

**MISSING:** Look back to Stargazer!

```{r, message=FALSE}

wb11_model<-rbind(cbind(fpLL="Pfal",
                        format(t(as.data.frame(wb11.model$coefficients[c(1,3,5,7)])), 
                                           digits = 2, scientific = FALSE)),
                  cbind(fpLL="Pviv",
                        format(t(as.data.frame(wb11.model$coefficients[c(2,4,6,8)])), 
                                           digits = 2, scientific = FALSE)))
colnames(wb11_model) <- c("drc 4pLL","Slope", "Lower.Limit", "Upper.Limit", "Infl.Point")
rownames(wb11_model) <- NULL#c("Pfal", "Pviv")
kable(wb11_model)

#ED(wb11.model,50, interval = "delta")

################################################

wb11_nls<-rbind(cbind(modl="Pfal",
                      format(cbind(t(as.data.frame(summary(dataPf.LL)$coefficients[c(4,1,2,3),1])),
                                               exp(summary(dataPf.LL)$coefficients[3,1])),
                                         digits = 2, scientific = FALSE)),
                cbind(modl="Pviv",
                      format(cbind(t(as.data.frame(summary(dataPv.LL)$coefficients[c(4,1,2,3),1])), 
                                               exp(summary(dataPv.LL)$coefficients[3,1])),
                                         digits = 2, scientific = FALSE)))
colnames(wb11_nls) <- c("nls 4pLL","Slope", "Lower.Limit", "Upper.Limit", "log.Infl.Point","Infl.Point")
rownames(wb11_nls) <- NULL#c("Pfal", "Pviv")
kable(wb11_nls[,-5])

#kable(cbind(Pfal.Pviv="nls", Slope.Pviv, Upper.Limit.Pviv, log.Infl.Point.Pviv, Infl.Point.Pviv))

################################################
wb11_model_BX<-rbind(cbind(fpLL="Pfal",
                           format(t(as.data.frame(wb11.model.BX$coefficients[c(1,3,5,7)])), 
                                           digits = 2, scientific = FALSE)),
                  cbind(fpLL="Pviv",
                        format(t(as.data.frame(wb11.model.BX$coefficients[c(2,4,6,8)])), 
                                           digits = 2, scientific = FALSE)))
colnames(wb11_model_BX) <- c("drc BoxCox","Slope", "Lower.Limit", "Upper.Limit", "Infl.Point")
rownames(wb11_model_BX) <- NULL#c("Pfal", "Pviv")
kable(wb11_model_BX)

#ED(wb11.model.BX,50, interval = "delta")

```


## Estimation of Ab units

Previously, How many UNK are out of the lower and upper limit of 4pLL and BoxCox models?

For the **4pLL** models, **`r sum(wb11_MEAN[1:n,5] < wb11.model$coefficients[3] | wb11_MEAN[1:n,5] > wb11.model$coefficients[5], na.rm = TRUE)` UNK** are either below the Lower.Limit or above the Upper.Limit of P.falciparum parameters, and **`r sum(wb11_MEAN[(n+1):(2*n),5] < wb11.model$coefficients[4] | wb11_MEAN[(n+1):(2*n),5] > wb11.model$coefficients[6], na.rm = TRUE)` UNK** for P.vivax. 

Similarly, for the **4pLL Box-Cox transformed** models, **`r sum(wb11_MEAN[1:n,5] < wb11.model.BX$coefficients[3] | wb11_MEAN[1:n,5] > wb11.model.BX$coefficients[5], na.rm = TRUE)` UNK** for P.falciparum parameters, and **`r sum(wb11_MEAN[(n+1):(2*n),5] < wb11.model.BX$coefficients[4] | wb11_MEAN[(n+1):(2*n),5] > wb11.model.BX$coefficients[6], na.rm = TRUE)` UNK** for P.vivax.


```{r, eval=FALSE}
#Pfal
sum(wb11_MEAN[1:n,5] < wb11.model$coefficients[3] | wb11_MEAN[1:n,5] > wb11.model$coefficients[5], na.rm = TRUE) 
#Pviv
sum(wb11_MEAN[(n+1):(2*n),5] < wb11.model$coefficients[4] | wb11_MEAN[(n+1):(2*n),5] > wb11.model$coefficients[6], na.rm = TRUE) 


#Pfal
sum(wb11_MEAN[1:n,5] < wb11.model.BX$coefficients[3] | wb11_MEAN[1:n,5] > wb11.model.BX$coefficients[5], na.rm = TRUE) 
#Pviv
sum(wb11_MEAN[(n+1):(2*n),5] < wb11.model.BX$coefficients[4] | wb11_MEAN[(n+1):(2*n),5] > wb11.model.BX$coefficients[6], na.rm = TRUE) 

```


From a given list of OD's, an output list of Ab units is presented. Std.Error and 95%CI are also displayed:
```{r}
wb11_Resp.Pf <- ED(wb11.model, 
                   wb11_MEAN[1:n,5],#wb11_MEAN$mean.OD, 
                   #subset(wb11_MEAN, Specie=="Pfal", select = "mean.OD"), 
                   type = "absolute",interval = "delta",
                   clevel = "Pfal", display = FALSE)
#wb11_Resp.Pf
wb11_Resp_Pf <- format(wb11_Resp.Pf, scientific = FALSE, digits = 1)
wb11_Resp_Pf


wb11_Resp.Pv <- ED(wb11.model, 
                   wb11_MEAN[(n+1):(2*n),5],#wb11_MEAN$mean.OD, 
                   #subset(wb11_MEAN, Specie=="Pviv", select = "mean.OD"), 
                   type = "absolute",interval = "delta",
                   clevel = "Pviv", display = FALSE)
#wb11_Resp.Pv
wb11_Resp_Pv <- format(wb11_Resp.Pv, scientific = FALSE, digits = 1)
wb11_Resp_Pv

```

### Using Box-Cox transformed model
```{r, fig.width=6, fig.height=4, fig.align='center', eval=TRUE}

## **BoxCox transformed [with error]**
##*Error in parmVec[3] - respl : non-numeric argument to binary operator*

wb11_Resp.Pf <- ED(wb11.model.BX, 
                   wb11_MEAN[1:n,5],#wb11_MEAN$mean.OD, 
                   #subset(wb11_MEAN, Specie=="Pfal", select = "mean.OD"), 
                   type = "absolute",interval = "delta",
                   clevel = "Pfal", display = FALSE)
#wb11_Resp.Pf
wb11_Resp_Pf <- format(wb11_Resp.Pf, scientific = FALSE, digits = 1)
wb11_Resp_Pf


wb11_Resp.Pv <- ED(wb11.model.BX, 
                   wb11_MEAN[(n+1):(2*n),5],#wb11_MEAN$mean.OD, 
                   #subset(wb11_MEAN, Specie=="Pviv", select = "mean.OD"), 
                   type = "absolute",interval = "delta",
                   clevel = "Pviv", display = FALSE)
#wb11_Resp.Pv
wb11_Resp_Pv <- format(wb11_Resp.Pv, scientific = FALSE, digits = 1)
wb11_Resp_Pv

```



### Visualization

- Required to add segments of *95%CI* on summary plot?

```{r, fig.width=6, fig.height=4, fig.align='center', eval=FALSE}
#par(mfrow=c(1,2))
plot(wb11.model,#broken = TRUE,
     ylim = c(0,1.5), 
     xlim = c(0,5e3),
     main="Standard Curve \n with estimated Ab units",
     col="grey",
     legendPos = c(2,1.5))
points(y=wb11_MEAN[(n+1):(2*n),5],x=wb11_Resp.Pv[1:n],col="blue",pch=4)
points(y=wb11_MEAN[1:n,5],x=wb11_Resp.Pf[1:n],col="red",pch=4)
abline(h=c(ctrNeg.Pfal, ctrNeg.Pviv), lty=c(3, 3), col=c("red", "blue"))
abline(h=c(ctrPlu.Pfal, ctrPlu.Pviv), lty=c(3, 3), col=c("red", "blue"))

#abline(h=c(0.14250, 1.282), lty=2)
```

```{r, fig.width=6, fig.height=4, fig.align='center'}
plot(wb11.model.BX,#broken = TRUE,
     ylim = c(0,1.5), 
     xlim = c(0,5e3),
     main="Standard Curve \n after Box-Cox transformation",
     col="grey",
     legendPos = c(2,1.5))
points(y=wb11_MEAN[(n+1):(2*n),5],x=wb11_Resp.Pv[1:n],col="blue",pch=4)
points(y=wb11_MEAN[1:n,5],x=wb11_Resp.Pf[1:n],col="red",pch=4)
abline(h=c(ctrNeg.Pfal, ctrNeg.Pviv), lty=c(3, 3), col=c("red", "blue"))
abline(h=c(ctrPlu.Pfal, ctrPlu.Pviv), lty=c(3, 3), col=c("red", "blue"))
```


About negative controls: 

- Among the *P.falciparum* unknown duplicated samples, `r sum(wb11_MEAN[wb11_MEAN$Specie=="Pfal",5] < ctrNeg.Pfal, na.rm = TRUE)` of `r length(wb11_MEAN[wb11_MEAN$Specie=="Pfal",5])` (`r (sum(wb11_MEAN[wb11_MEAN$Specie=="Pfal",5] < ctrNeg.Pfal, na.rm = TRUE)/length(wb11_MEAN[wb11_MEAN$Specie=="Pfal",5]))*100`%) have lower mean OD values than the Negative Control  (OD=`r ctrNeg.Pfal`).

- In the case of *P.vivax*, `r sum(wb11_MEAN[wb11_MEAN$Specie=="Pviv",5] < ctrNeg.Pviv, na.rm = TRUE)` of `r length(wb11_MEAN[wb11_MEAN$Specie=="Pviv",5])` (`r (sum(wb11_MEAN[wb11_MEAN$Specie=="Pviv",5] < ctrNeg.Pviv, na.rm = TRUE)/length(wb11_MEAN[wb11_MEAN$Specie=="Pviv",5]))*100`%) duplicated unknown samples have lower mean OD values than the Negative Control (OD=`r ctrNeg.Pviv`).

```{r}
# Check the values here:
##subset(wb11_ALL, Type==c("std","ctr"))
#subset(wb11_ALL, ID=="Blank")
#subset(wb11_ALL, Type=="ctr")
```

Finally, a descriptive statistical summary of the mean OD per specie is given:
```{r}
#str(wb11_MEAN)
summary(subset(wb11_MEAN, Specie=="Pfal", select = c("mean.OD","Specie")))
summary(subset(wb11_MEAN, Specie=="Pviv", select = c("mean.OD","Specie")))
#relpot(wb11.model, interval = "delta")
```

### Control +/- Ab.unit Estimation
```{r}
subset(wb11_ALL, Type=="ctr")

wb11_Ctrl.Pf <- ED(wb11.model.BX, 
                   wb11_ALL[wb11_ALL$Type=="ctr" & wb11_ALL$Specie=="Pfal",5],#wb11_MEAN$mean.OD, 
                   #subset(wb11_MEAN, Specie=="Pfal", select = "mean.OD"), 
                   type = "absolute",interval = "delta",
                   clevel = "Pfal", display = FALSE)
#wb11_Ctrl.Pf

wb11_Ctrl.Pv <- ED(wb11.model.BX, 
                   wb11_ALL[wb11_ALL$Type=="ctr" & wb11_ALL$Specie=="Pviv",5],#wb11_MEAN$mean.OD, 
                   #subset(wb11_MEAN, Specie=="Pfal", select = "mean.OD"), 
                   type = "absolute",interval = "delta",
                   clevel = "Pviv", display = FALSE)
#wb11_Ctrl.Pv
```

```{r}
wb11_ALL[wb11_ALL$Type=="std" & wb11_ALL$ID=="STD 1/200" | wb11_ALL$ID=="Blank",]
wb11_Ctrl.Pf
wb11_Ctrl.Pv
```


### Assign values to the table

From the table above:
```{r}
#head(wb11_MEAN)
head(wb11_SDCV)
```

Empty spaces of Ab units per sample are filled:
```{r}
for (i in 1:n) {
  wb11_SDCV[i,4] <- wb11_Resp.Pf[i]
  wb11_SDCV[n+i,4] <- wb11_Resp.Pv[i]
}

head(wb11_SDCV)

#wb11_SDCV_NOab <- wb11_SDCV
```
A full display of this table is in the [Appendix D](#appendix-d-data-frame-per-steps).


### OD vs Ab unit distribution

2x OD replicates -> mean(OD) -> Ab.units estimation

```{r, fig.align='center', fig.height=9, fig.width=9}
par(mfrow=c(3,3))

my <- max(hist(wb11_ALL$OD, plot = F)$counts, na.rm=TRUE)
#
hist(wb11_ALL$OD, ylim = c(0,my))
plot(density(na.omit(wb11_ALL$OD)))#, ylim = c(0,my)  #, xlim = c(0,mx)
qqnorm(wb11_ALL$OD);qqline(wb11_ALL$OD)

hist(wb11_MEAN$mean.OD, ylim = c(0,my))
plot(density(na.omit(wb11_MEAN$mean.OD)))#, ylim = c(0,my)
qqnorm(wb11_MEAN$mean.OD);qqline(wb11_MEAN$mean.OD)

hist(wb11_SDCV$Ab.unit, ylim = c(0,my))
plot(density(na.omit(wb11_SDCV$Ab.unit)))#, ylim = c(0,my)
qqnorm(wb11_SDCV$Ab.unit);qqline(wb11_SDCV$Ab.unit)

```



## Output: **.csv files**

Three new files are going to be present in the working directory.

1. A table of Standardized Ab units
```{r}
#write.csv(wb11_SDCV, "Standardized_ELISA_N11.csv")
```

2. A table of mean.blank.OD and pseudo-R-squared required for Quality Control plot
```{r}
bkpf <- blank.Pfal
R.sq_Pfal <- format(R.sq_Pfal, digits = 4)
R.sq_Pfal_BX <- format(R.sq_Pfal_BX, digits = 4)
R.CS_Pfal <- format(nagelkerke(dataPf.LL, null = null.model)$Pseudo.R.squared.for.model.vs.null[2]
                    , digits = 4)
nkpf <- rbind(R.CS_Pfal,R.sq_Pfal,R.sq_Pfal_BX)

bkpv <- blank.Pviv
R.sq_Pviv <- format(R.sq_Pviv, digits = 4)
R.sq_Pviv_BX <- format(R.sq_Pviv_BX, digits = 4)
R.CS_Pviv <- format(nagelkerke(dataPv.LL, null = null.model)$Pseudo.R.squared.for.model.vs.null[2]
                    , digits = 4)
nkpv <- rbind(R.CS_Pviv,R.sq_Pviv,R.sq_Pviv_BX)


wb11_BKR2 <- rbind(cbind(Plate="N11", Specie="Pfal", blank=bkpf, 
                         R.square=nkpf, type=c("CS","R2","R2.BX")), 
                   cbind(Plate="N11", Specie="Pviv", blank=bkpv, 
                         R.square=nkpv, type=c("CS","R2","R2.BX")))
colnames(wb11_BKR2) <- c("Plate", "Specie", "Blank","R.square","Type")
rownames(wb11_BKR2) <- NULL
#write.csv(wb11_BKR2, "QualityCTRL_ELISA_N11.csv")

kable(wb11_BKR2, align = "l")
```


3. A table of STD curve OD values
```{r}
#write.csv(subset(wb11_ALL, Type=="std"), "QualitySTD_ELISA_N11.csv")
```

***

# APPENDIX A: From OD to Ab units

- Sepulveda et.al. [@Seplveda2015] cite two main publications:
    1. **Cook et.al., 2011 in Bioko Island, Equatorial Guinea** [@Cook2011], and
    2. **Cunha et.al., 2014 in Jacareacanga, Brazil** [@Cunha2014].
- Both papers followed the same experimental procedure to standardize the data across plates. Althought, non of them cite **Miura et.al., 2008**[@Miura2008].
- Nuno Sepulveda is co-author of the second paper [@Cunha2014].


## NAMRU-6 method

The method provided by Julio followed this procedure, making two main assumptions:

1. An **endpoint titer** equal to the last dilution of the STD, in this case `r format(x[length(x)], scientific=FALSE)`.
2. An equal **endpoint titer** among *P.falciparum* and *P.vivax* STD's.

```{r}
example <- cbind(unlist(Pf.ID[1:12,]), 
                 x, 
                 gsub('STD 1','102400',unlist(Pf.ID[1:12,])),
                 x[length(x)]/x)
colnames(example) <- c("Dilution", "Reciprocal of Dilution", "Factor", "ELISA Ab units")
kable(example)
```


## K.Miura method

K.Miura et.al. [@Miura2008] previously assigned Antibody Units to the STD (as the reciprocal dilution giving an OD~450nm~=1*). After this step, the STD was applied to each ELISA plate.

  (@) For example, for the **Anti-Pvs25 mokey sera**, after generating aliquotes of 1:100 from the initial pool, four-fold serial dilution was perfomed. Here, an OD~450nm~=1 had a Rec.Dilution of 20,000. Then STD Ab units was equal to 20,000. 

  (@) After the assignation of Ab units to the STD, a two-fold serial dilution starting with a dilution of 20 Ab units, equivalent to 1:1000 dilution, was applied on each ELISA plate.

  (@) As an example, in the paper is said that the 5th dilution of the STD sera is suppoused to have 1.25 Ab units. So, if $1.25=\frac{STD}{16000}$ then the STD Ab units are equal to 20,000.

*The reason behind an OD~450nm~=1 may be in the variability of the STD curve Upper and Lower limits. This behaviour also is observed in the comparoson of the STD OD values across ELISA plates.

## Corran method

Cook et.al.[@Cook2011] cite Corran et.al. [@Corran2008] which have a full description of the methods.

Corran et.al.[@Corran2008] make the assumption of an undiluted (initial) concentration of the sera pool previous to a 3pLL model fitting. By this way, the OD is directly proportional to the independent variable. [suppl.file 1](https://static-content.springer.com/esm/art%3A10.1186%2F1475-2875-7-195/MediaObjects/12936_2008_670_MOESM1_ESM.pdf)

At Methods, they write the following:

> "A titration curve was fitted to the ODs obtained for the standard plasma dilutions by least squares minimisation using a three variable sigmoid model and the solver add-in in Excel (Microsoft), assuming an arbitrary value of **1000 Units/ml** of antibody against each antigen in the **standard pool**. OD values for the spot extracts were converted to units/ml using this fitted curve."

***
  
# APPENDIX B: the 4pLL model

The **four-parameter log-logistic function** have the following presentations [@weimer2012impact]: $$f(x)=f(x;b,c,d,e)=c+\frac{d-c}{1+\exp[b(log(x)-log(e))]}=c+\frac{d-c}{1+\left(\frac{x}{e}\right)^b}=c+\frac{d-c}{1+{10}^{b(log_{10}(x)-log_{10}(e))}}$$

By isolating $x$, equivalent expressions are commonly reported as inverse functions of the 4pLL equation, assuming `b=-1` and then `c=0`:
$$x=e\left[{\left(\frac{d-f(x)}{f(x)-c}\right)}^{1/b}\right] \qquad \Rightarrow \qquad x=\frac{e}{\frac{d-c}{f(x)-c}-1} \qquad  \Rightarrow \qquad x=\frac{e}{\frac{d}{f(x)}-1}$$

If no log-transformation is required, then the following expression are often employed, depending on the scale of `x`: $$f(x)=c+\frac{d-c}{1+\exp[b(x-e)]} \qquad or \qquad f(x)=c+\frac{d-c}{1+{10}^{b(x-e)}}$$ For example, this last expression, with `x` in decimal-log scale, is present in the page 7 of the *drLumi* [@drLumi] package vignette.

Lastly, when using the nls function to fit the data, [Self-Starter](https://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Nonlinear-Regression.pdf) functions are required. This functions, in contrast with the last example, requires `x` in natural-log scale. E.g. the logistic model have the following parameters $$f(x)=\frac{\phi_1}{1+\exp\left[ \frac{\phi_2-x}{\phi_3}\right]} \qquad \Rightarrow \qquad f(x)=0+\frac{\phi_1-0}{1+\exp\left[ \frac{-1}{\phi_3} (x-\phi_2) \right]} $$

## Publications

  1. **Miura et.al, 2008** [@Miura2008] reported a "four-parameter hyperbolic curve" in this form: $$x=e\left[{\left(\frac{d-f(x)}{f(x)-c}\right)}^{1/b}\right]$$ where $x={Ab}_{units}$ and $f(x)={OD}_{450nm}$ without parameter letters definition. Those parameter letters have been changed in order to keep concordance with the extended description gave [above](#parameter-estimation).

  1. **Cook et.al., 2011 in Bioko Island, Equatorial Guinea** [@Cook2011] cite Corran et.al. [@Corran2008] which have a full description of the methods [suppl.file 1](https://static-content.springer.com/esm/art%3A10.1186%2F1475-2875-7-195/MediaObjects/12936_2008_670_MOESM1_ESM.pdf). Briefly, they report a ligand-binding equation, with the following inverse function: $$A=\frac{A_{max}*B}{B+k} \qquad \Rightarrow \qquad B=\frac{C}{D}=\frac{k}{\frac{A_{max}}{A}-1}$$ That equation takes a recognizable form after a rearrangement, showing a 4pLL assuming `c=0` and `b=-1`, the same as with the inverse form: $$A=0+\frac{A_{max}-0}{1+\left(\frac{B}{k}\right)^{-1}} \equiv f(x)=0+\frac{d-0}{1+\left(\frac{x}{e}\right)^b}  \qquad and \qquad B=\frac{k}{\frac{A_{max}-0}{A-0}-1} \equiv x=\frac{e}{\frac{d-0}{f(x)-0}-1} $$
    
  2. **Cunha & Sepulveda et.al., 2014 in Jacareacanga, Brazil** [@Cunha2014] report a formula without any details, as in the cited publication [@Bousema2010]: $$ {titer}= \frac{dilution}{ \frac{maximum.OD}{OD.test.serum - minimum.OD} -1} \qquad \neq \qquad x=\frac{e}{\frac{d-c}{f(x)-c}-1} $$ Eventually, they tried to write the inverse of the 4pLL model but fail in the description of the variables.
  

**CONCLUSION:**

Here I have shortly compare the equivalent expressions applied by several reference publications [@Miura2008] [@Cook2011] [@Cunha2014] [@Corran2008], all equal to the **four-parameter log-logistic function** applied in this report.

***

# APPENDIX C: R square

## OLS R-square

R-square definition for Ordinary Least Square (OLS) method for parameter estimation in Linear Models [@anderson1994model]:

$$r^2= \frac{\sum_{i=1}^n(\hat{y_i} - \bar{y_i})^2}{\sum_{i=1}^n(y_i - \bar{y})^2} = 1-\frac{\sum_{i=1}^n(y_i - \hat{y_i})^2}{\sum_{i=1}^n(y_i - \bar{y})^2} \qquad \Rightarrow \qquad {GR}^2= 1-\frac{RSS(full)}{RSS(reduced)} = 1-\frac{Test.model}{Null.model}$$

Three approaches for R-square [(LINK)](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm):

- explained variability
- improvement from Null model to Fitted model
- As squared of correlation

## pseudo R-square

However, this R-square for Linear regressions may be misleading for Nonlinear ones. For two reasons:

- The method employed in the parameter estimation is different from the OLS.
    + According to Ritz, et.al.[@Ritz2015], **drc** estimation of parameters is based on Maximum Likelihood principle, which under the assumption of normally distributed response values simplifies to Nonlinear Least Square.

- Under the context of a **Generalized R-square**[@anderson1994model], as a model comparison against a null model, Nonlinear models would require the selection of an arbitrary Null model, generating different R-squares depending on the chosen one. [(LINK)](http://rstats4ag.org/dose-response/#Note_on_use_of_R2)
    + Alternatives to this are **pseudo R-squares**, based on Likelihood results, which:
        * requires a Null model
        * values could be out of 0-1 range
        * used in categorical logistic regression

## Generalized R-square

Lastly, if a **GR-square** [@anderson1994model] is based on model comparison against a null one, then Nonlinear regression output **LogLik** parameter should be compared by **LRT**, **AIC** and **residuals variability**.

Here, a Model selection is performed by comparison of different models using the following criteria: 

- the log likelihood value, 
- Akaike's information criterion (AIC), 
- the estimated residual standard error or 
- the p-value from a lack-of-fit test.

In this case, a *lack-of-fit* test via ANOVA is not reliable since we only have more than one point per dilution in the blank STD.

A significance test (LRT) is also provided for the comparison of the dose-response model considered and the simple linear regression model with slope 0 (a horizontal regression line corresponding to no dose effect), a **Null model**.

**model selection criterias:**

- **AIC** recommended to test Nonlinear models: ${AIC}=2k-2log(L)$
- **LRT** for comparison of LogLik estimates between 2 Nested models as a ${\chi}^2$ test of homogeneity: ${LRT}=-2log(\frac {L_s}{L_g} )$


***

# APPENDIX D: data frame per steps

## full **raw** data frame
A full visualization of the raw data frame:
```{r}
wb11_RAW
```

## full **raw** mean data

A full display of this table with the **unsubtracted** mean values filled: **only for comparison with .xlsx*
```{r}
wb11_MEAN_NO_SUB
```

## full **subtracted** data frame
If subtraction is **[ON]**, then a full visualization of the subtracted data frame is display:
```{r}
if (sum(wb11_RAW$OD, na.rm=TRUE) != sum(wb11_ALL$OD, na.rm=TRUE)) {
   wb11_RAW_SUB
}
```

## full **subtracted** mean data
If subtraction is **[ON]**, then a full visualization of the subtracted data is display:

```{r}
if (sum(wb11_MEAN_NO_SUB$mean.OD, na.rm=TRUE) != sum(wb11_MEAN$mean.OD, na.rm=TRUE)) {
   wb11_SUB
}
```


## full table with SD and CV values
A full display of this table with the mean, SD and CV values filled:
```{r}
wb11_SDCV_PRE
```

## full table with Ab.unit values
A full display of this table with the mean, SD, CV and Ab.unit values filled:
```{r}
wb11_SDCV
```

***
  
# Computer environment

```{r}
sessionInfo()
```


# References
